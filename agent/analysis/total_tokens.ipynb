{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30357473,\n",
       " 975658,\n",
       " 31333131,\n",
       "                     Model Cost (USD)\n",
       " 0                  gpt-4o    $166.42\n",
       " 6          Llama-3.1-405B    $156.67\n",
       " 7  gemini-1.5-pro (short)    $116.50\n",
       " 3       claude-3.5-sonnet    $105.71\n",
       " 5           Llama-3.1-70B     $27.57\n",
       " 2            mistral-nemo      $9.40\n",
       " 4          claude-3-haiku      $8.81\n",
       " 1             gpt-4o-mini      $5.14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import ast\n",
    "\n",
    "from masterthesis.agent.GitAgent import GitAgent\n",
    "from masterthesis.agent.MavenReproducerAgent import MavenReproducerAgent\n",
    "from masterthesis.dataset.load_dataset import cleanup_dataset, load_dataset\n",
    "\n",
    "def load_persisted_str_data(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    with file_path.open('r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Basic safety check: ensure the content is a valid Python expression\n",
    "    try:\n",
    "        ast.parse(content)\n",
    "    except SyntaxError:\n",
    "        raise ValueError(\"The file content is not a valid Python expression\")\n",
    "    \n",
    "    # Use eval to evaluate the string\n",
    "    data = eval(content, {\n",
    "        'AIMessage': AIMessage,\n",
    "        'HumanMessage': HumanMessage,\n",
    "        'SystemMessage': SystemMessage,\n",
    "        'ToolMessage': ToolMessage\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_language_model(file_path):\n",
    "    return file_path.split(\"/\")[-2]\n",
    "\n",
    "# Get all agent protocol files\n",
    "agent_protocol_files = glob.glob(\"dataset/*/*/agent_protocol.json\")\n",
    "\n",
    "# Initialize dictionaries to store data for each language model\n",
    "data = defaultdict(lambda: {'Attempts': 0, 'Compilation Errors': 0, 'Compile Success': 0, 'Test Success': 0})\n",
    "\n",
    "# total_input_tokens =0\n",
    "# total_output_tokens = 0\n",
    "\n",
    "\n",
    "reproduced_compilation_success = 0\n",
    "reproduced_test_success = 0\n",
    "\n",
    "# for file in agent_protocol_files:\n",
    "#     lm = get_language_model(file)\n",
    "#     commit_hash = file.split(\"/\")[-3]\n",
    "#     data[lm]['Attempts'] += 1\n",
    "\n",
    "\n",
    "#     final_state = load_persisted_str_data(Path(file).parent / \"final_state\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     for message in final_state[\"messages\"]:\n",
    "#         if isinstance(message, AIMessage):\n",
    "           \n",
    "#            if message.usage_metadata:\n",
    "#             total_input_tokens += message.usage_metadata['input_tokens']\n",
    "#             total_output_tokens += message.usage_metadata['output_tokens']\n",
    "\n",
    "\n",
    "# total_input_tokens = 30544504\n",
    "# total_output_tokens = 768981\n",
    "# num_invocations = 1800\n",
    "\n",
    "total_input_tokens = 30357473\n",
    "total_output_tokens = 975658\n",
    "num_invocations = 1578\n",
    "\n",
    "\n",
    "with open(\"model_metadata.json\", \"r\") as file:\n",
    "    model_metadata = json.load(file)\n",
    "\n",
    "with open(\"model_prices.json\", \"r\") as file:\n",
    "    prices = json.load(file)\n",
    "\n",
    "\n",
    "# Calculate the average input and output tokens per invocation\n",
    "average_input_tokens_per_invocation = total_input_tokens // num_invocations\n",
    "average_output_tokens_per_invocation = total_output_tokens // num_invocations\n",
    "\n",
    "\n",
    "total_rounded_input_tokens = total_input_tokens\n",
    "total_rounded_output_tokens = total_output_tokens\n",
    "\n",
    "# Calculate total tokens per day\n",
    "total_tokens_per_day = total_rounded_input_tokens + total_rounded_output_tokens\n",
    "\n",
    "# Calculate costs for each model with rounded tokens\n",
    "rounded_costs = {}\n",
    "\n",
    "# Models with separate input and output prices\n",
    "for model in [\"gpt-4o\", \"gpt-4o-mini\", \"mistral-nemo\"]:\n",
    "    input_cost = (total_rounded_input_tokens / 1_000_000) * prices[model][\"input\"]\n",
    "    output_cost = (total_rounded_output_tokens / 1_000_000) * prices[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    rounded_costs[model] = total_cost\n",
    "\n",
    "# Add Claude models only if total tokens per day is 1 million or lower\n",
    "# if total_tokens_per_day <= 1_000_000:\n",
    "for model in [\"claude-3.5-sonnet\", \"claude-3-haiku\"]:\n",
    "    input_cost = (total_rounded_input_tokens / 1_000_000) * prices[model][\"input\"]\n",
    "    output_cost = (total_rounded_output_tokens / 1_000_000) * prices[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    rounded_costs[model] = total_cost\n",
    "\n",
    "# Models with combined input and output prices\n",
    "for model in [\"Llama-3.1-70B\", \"Llama-3.1-405B\"]:\n",
    "    combined_tokens = total_rounded_input_tokens + total_rounded_output_tokens\n",
    "    combined_cost = (combined_tokens / 1_000_000) * prices[model][\"combined\"]\n",
    "    rounded_costs[model] = combined_cost\n",
    "\n",
    "for model in [\"gemini-1.5-pro\"]:\n",
    "    input_cost_short = (total_rounded_input_tokens / 1_000_000) * prices[model][\"input_short\"]\n",
    "    output_cost_short = (total_rounded_output_tokens / 1_000_000) * prices[model][\"output_short\"]\n",
    "    total_cost_short = input_cost_short + output_cost_short\n",
    "\n",
    "    input_cost_long = (total_rounded_input_tokens / 1_000_000) * prices[model][\"input_long\"]\n",
    "    output_cost_long = (total_rounded_output_tokens / 1_000_000) * prices[model][\"output_long\"]\n",
    "    total_cost_long = input_cost_long + output_cost_long\n",
    "\n",
    "    if average_input_tokens_per_invocation <= 500_000:\n",
    "        rounded_costs[f\"{model} (short)\"] = total_cost_short\n",
    "    else:\n",
    "    # rounded_costs[f\"{model} (short)\"] = total_cost_short\n",
    "        rounded_costs[f\"{model} (long)\"] = total_cost_long\n",
    "\n",
    "# Display the results in a readable format\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "costs_df = pd.DataFrame(list(rounded_costs.items()), columns=[\"Model\", \"Cost (USD)\"])\n",
    "costs_df.sort_values(\"Cost (USD)\", ascending=False, inplace=True)\n",
    "costs_df[\"Cost (USD)\"] = costs_df[\"Cost (USD)\"].apply(lambda x: f\"${x:.2f}\")\n",
    "\n",
    "# Display rounded tokens and costs\n",
    "total_rounded_input_tokens, total_rounded_output_tokens, total_tokens_per_day, costs_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
