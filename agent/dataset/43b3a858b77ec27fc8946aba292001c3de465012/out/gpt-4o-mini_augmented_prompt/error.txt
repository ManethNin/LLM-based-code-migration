Traceback (most recent call last):
  File "/root/thesis/masterthesis-implementation-gpt/langchain-agent.py", line 948, in <module>
    first_shot_state = app.invoke(
                       ^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py", line 1554, in invoke
    for chunk in self.stream(
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py", line 1073, in stream
    _panic_or_proceed(done, inflight, step)
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py", line 1643, in _panic_or_proceed
    raise exc
  File "/usr/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py", line 72, in run_with_retry
    task.proc.invoke(task.input, task.config)
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 2502, in invoke
    input = step.invoke(input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langgraph/utils.py", line 95, in invoke
    ret = context.run(self.func, input, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/langchain-agent.py", line 899, in call_model
    response = llm_with_tools.invoke(messages)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py", line 4573, in invoke
    return self.bound.invoke(
           ^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 170, in invoke
    self.generate_prompt(
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 599, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 456, in generate
    raise e
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 446, in generate
    self._generate_with_cache(
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py", line 671, in _generate_with_cache
    result = self._generate(
             ^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py", line 547, in _generate
    response = self.client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 277, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 643, in create
    return self._post(
           ^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1266, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 942, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/root/thesis/masterthesis-implementation-gpt/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1046, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 128000 tokens. However, your messages resulted in 132753 tokens (132437 in the messages, 316 in the functions). Please reduce the length of the messages or functions.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
