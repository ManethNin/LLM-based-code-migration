{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "# Filter for LLM spans\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# language_model_id = \"gpt-4o-mini_baseline\"\n",
    "# language_model_id = \"open-mistral-nemo_dspy-baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_from_results(error_txt, crop_until_results: bool = False):\n",
    "  split=error_txt.split(\"\\n\")\n",
    "  idx = 0\n",
    "  has_found_results = False\n",
    "  end_idx = len(split)-1\n",
    "  for i, line in enumerate(split):\n",
    "    if \"[INFO] Results:\" in line and not has_found_results:\n",
    "        idx= i\n",
    "        has_found_results = True\n",
    "    if \"Tests run:\" in line and crop_until_results:\n",
    "        end_idx = i\n",
    "\n",
    "  return \"\\n\".join(split[idx:end_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grouped_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Filter for LLM spans\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm_df \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_df\u001b[49m[grouped_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopeninference.span.kind\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Group data by trace_id\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trace_groups \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grouped_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Filter for LLM spans\n",
    "llm_df = grouped_df[grouped_df[\"openinference.span.kind\"] == \"LLM\"]\n",
    "\n",
    "# Group data by trace_id\n",
    "trace_groups = defaultdict(list)\n",
    "\n",
    "for _, row in llm_df.iterrows():\n",
    "    trace_id = row[\"trace_id\"]\n",
    "    commit_hash = row[\"commit_hash\"]\n",
    "    language_model = row[\"language_model\"]\n",
    "\n",
    "    trace_groups[trace_id].append(\n",
    "        {\n",
    "            \"commit_hash\": commit_hash,\n",
    "            \"language_model\": language_model,\n",
    "            \"data\": row.where(pd.notna(row), None).to_dict(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Write grouped data to files\n",
    "for trace_id, group_data in trace_groups.items():\n",
    "    if not group_data:\n",
    "        continue\n",
    "\n",
    "    commit_hash = group_data[0][\"commit_hash\"]\n",
    "    language_model = group_data[0][\"language_model\"]\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    directory = f\"dataset/{commit_hash}/out/llm/{language_model}/\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Create filename using trace_id\n",
    "    filename = f\"{directory}{trace_id}.json\"\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(group_data, f, default=str, indent=2)\n",
    "\n",
    "print(\"Files have been written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from masterthesis.evaluation.output_success_criterion import output_success_criterion\n",
    "import os\n",
    "\n",
    "\n",
    "# find dataset/*/out/{language_model_id}-*-execution-errors.json \n",
    "data_path = Path(os.path.abspath(\"\"))/\"dataset\"\n",
    "\n",
    "execution_details = {}\n",
    "\n",
    "\n",
    "total_error_count = 0\n",
    "total_diff_error_count = 0\n",
    "\n",
    "total_full_successes = 0\n",
    "total_compiled = 0\n",
    "\n",
    "\n",
    "total_attempts_compiled_count = 0\n",
    "total_attempts_not_compiled_count = 0\n",
    "\n",
    "trial_data = defaultdict(dict)\n",
    "\n",
    "\n",
    "language_model_id = \"claude-3-haiku@20240307\"\n",
    "\n",
    "\n",
    "for path in data_path.rglob(f\"out/full-{language_model_id}*-execution-error*.json\"):\n",
    "    # I want the commit hash and the number between language model and execution-errors\n",
    "    commit_hash = path.parts[-3]\n",
    "    # experiment_number = path.parts[-1].split(\"-execution-errors.json\")[0]\n",
    "    # experiment_number = experiment_number.split(\"-\")[-1]\n",
    "    if \"execution-errors.json\" in path.parts[-1]:\n",
    "        experiment_number = \"trial-0\"\n",
    "    elif \"execution-errors-1.json\" in path.parts[-1]:\n",
    "        experiment_number = \"trial-1\"\n",
    "    elif \"execution-errors-2.json\" in path.parts[-1]:\n",
    "        experiment_number = \"trial-2\"\n",
    "\n",
    "    def add_to_trial_data(key:str, val: int):\n",
    "        if key not in trial_data[experiment_number]:\n",
    "            if isinstance(val, int):\n",
    "                trial_data[experiment_number][key] = 0\n",
    "            if isinstance(val, str):\n",
    "                trial_data[experiment_number][key] = []\n",
    "\n",
    "        if isinstance(val, int):\n",
    "            trial_data[experiment_number][key] += val\n",
    "        if isinstance(val, str):\n",
    "            trial_data[experiment_number][key].append(val)\n",
    "\n",
    "    # print(commit_hash, experiment_number)\n",
    "    with open(path, \"r\") as f:\n",
    "        execution_errors = json.load(f)\n",
    "        execution_errors = execution_errors.get(commit_hash, execution_errors)\n",
    "        if len(execution_errors.keys()) == 0:\n",
    "            continue\n",
    "        if commit_hash not in execution_details:\n",
    "            execution_details[commit_hash] = []\n",
    "        execution_details[commit_hash].append(execution_errors)\n",
    "\n",
    "        # print(execution_errors.keys())\n",
    "\n",
    "        # print(execution_errors)\n",
    "\n",
    "        if \"error\" in execution_errors and execution_errors['error'] is not None:\n",
    "            # print(\"plus one total errors\")\n",
    "            total_error_count += 1\n",
    "            add_to_trial_data(\"error_count\", 1)\n",
    "\n",
    "            if not \"hops\" in trial_data[experiment_number]:\n",
    "                trial_data[experiment_number][\"hops\"] = {}\n",
    "            trial_data[experiment_number][\"hops\"][commit_hash] = len(execution_errors.get(\"diff_attempts\", []))\n",
    "\n",
    "            add_to_trial_data(\"hop\", len(execution_errors.get(\"compiled_protocol\", [])))\n",
    "\n",
    "            if \"Invalid diff: [Errno 2] No such file or directory\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_hallucinated_diff_path\", 1)\n",
    "\n",
    "            if \"Unfixable JVM Error\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_unfixable_jvm_error\", 1)\n",
    "            \n",
    "            if \"Total tokens exceed the threshold\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_total_tokens_exceed_threshold\", 1)\n",
    "\n",
    "            if \"UnifiedDiffNoMatch: hunk failed to apply\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_unified_diff_no_match\", 1)\n",
    "\n",
    "            if \"No diff fences found in content.\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_no_diff_fences\", 1)\n",
    "\n",
    "            if \"UnifiedDiffNoEdits: no applicable hunks found\" in execution_errors['error']:\n",
    "                add_to_trial_data(\"error_details_unified_diff_no_edits\", 1)\n",
    "\n",
    "                \n",
    "        # total_diff_error_count += 1 if error_info is not None and \"Invalid diff\" in error_info else 0\n",
    "        if \"diff_attempts\" in execution_errors:\n",
    "            total_diff_error_count += len(execution_errors[\"diff_attempts\"])\n",
    "            add_to_trial_data(\"diff_error_count\", total_diff_error_count)\n",
    "        else:\n",
    "            print(\"no diff attempts\", commit_hash)\n",
    "            print(commit_hash, execution_errors)\n",
    "\n",
    "        if output_success_criterion(execution_errors.get(\"output\", None)) == True:\n",
    "            total_full_successes += 1\n",
    "            add_to_trial_data(\"full_successes\", 1)\n",
    "            add_to_trial_data(f\"full_successes-hash\", commit_hash)\n",
    "\n",
    "        if \"compiled_protocol\" in execution_errors and execution_errors[\"compiled_protocol\"] is not None:\n",
    "            for status in execution_errors[\"compiled_protocol\"]:\n",
    "                if status == True:\n",
    "                    total_attempts_compiled_count += 1\n",
    "                    add_to_trial_data(\"attempts_compiled\", 1)\n",
    "                else:\n",
    "                    total_attempts_not_compiled_count += 1\n",
    "                    add_to_trial_data(\"attempts_not_compiled\", 1)\n",
    "        else:\n",
    "            add_to_trial_data(\"no_compiled_protocol\", 1)\n",
    "        \n",
    "        if \"compiled\" in execution_errors:\n",
    "            if execution_errors[\"compiled\"] == True:\n",
    "                total_compiled += 1\n",
    "                add_to_trial_data(\"compiled\", 1)\n",
    "                add_to_trial_data(f\"compiled-hash\", commit_hash)\n",
    "\n",
    "                # repro_path = Path(os.path.abspath(\"\"))/f\"dataset/{commit_hash}/out/reproduction\"\n",
    "                # post_path = repro_path / \"post.txt\"\n",
    "                # pre_path = repro_path / \"pre.txt\"\n",
    "\n",
    "                # if repro_path.exists() and pre_path.exists() and post_path.exists():\n",
    "\n",
    "                #     # dataset/0305beafdecb0b28f7c94264ed20cdc4e41ff067/out/reproduction/post.txt\n",
    "                #     with open(f\"dataset/{commit_hash}/out/reproduction/post.txt\", \"r\") as f:\n",
    "                #         post_txt = crop_from_results(f.read(), False)\n",
    "\n",
    "                #     with open(f\"dataset/{commit_hash}/out/reproduction/pre.txt\", \"r\") as f:\n",
    "                #         pre_txt = crop_from_results(f.read(), False)\n",
    "\n",
    "                #     if \"[INFO] BUILD FAILURE\" in pre_txt:\n",
    "                #         add_to_trial_data(\"tests_broken_in_environment\", 1)\n",
    "                #     # print(\"equal\", pre_txt == post_txt)\n",
    "                #     # if pre_txt == post_txt:\n",
    "                #     #     add_to_trial_data(\"tests_broken_in_environment\", 1)\n",
    "                #     # else:\n",
    "                #     #     print(\"not equal\")\n",
    "                #     #     print(pre_txt)\n",
    "                #     #     print()\n",
    "                #     #     print(post_txt)\n",
    "                # else:\n",
    "                #     add_to_trial_data(\"no_repro\", 1)\n",
    "                    \n",
    "\n",
    "\n",
    "print(\"total_error_count\", total_error_count)\n",
    "print(\"total_diff_error_count\", total_diff_error_count)\n",
    "print(\"total_full_successes\", total_full_successes)\n",
    "print(\"total_compiled\", total_compiled)\n",
    "print(\"total_attempts_compiled_count\", total_attempts_compiled_count)\n",
    "print(\"total_attempts_not_compiled_count\", total_attempts_not_compiled_count)\n",
    "\n",
    "print(json.dumps(trial_data, indent=4))\n",
    "\n",
    "with open(language_model_id+\"_data.json\", \"w\") as f:\n",
    "    json.dump(trial_data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_and_delete_files_with_errors():\n",
    "    data_path = Path(os.path.abspath(\"\")) / \"dataset\"\n",
    "    error_messages = [\"403 This API method requires billing\", \"Quota exceeded\"]\n",
    "    commit_hashes_with_errors = set()\n",
    "\n",
    "    for path in data_path.rglob(\"out/*-execution-error*\"):\n",
    "        commit_hash = path.parts[-3]\n",
    "        with open(path, \"r\") as f:\n",
    "            execution_errors = json.load(f)\n",
    "            for key, value in execution_errors.items():\n",
    "                if isinstance(value, dict):\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if any(error in str(sub_value) for error in error_messages):\n",
    "                            commit_hashes_with_errors.add(commit_hash)\n",
    "                            os.remove(path)\n",
    "                            break\n",
    "                else:\n",
    "                    if any(error in str(value) for error in error_messages):\n",
    "                        commit_hashes_with_errors.add(commit_hash)\n",
    "                        os.remove(path)\n",
    "                        break\n",
    "\n",
    "    return commit_hashes_with_errors\n",
    "\n",
    "commit_hashes = find_and_delete_files_with_errors()\n",
    "print(commit_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Table LaTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_model_mapper(language_model_id):\n",
    "    language_model_id = language_model_id.replace(\"-execution-errors.json\", \"\")\n",
    "    language_model_id = language_model_id.replace(\".json\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"meta-llama_Meta-\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"-Instruct-Turbo\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"open-mistral\", \"mistral\")\n",
    "    language_model_id = language_model_id.replace(\"-001\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"text-bison@002\", \"text-bison-002\")\n",
    "    language_model_id = language_model_id.replace(\"@20240307\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"@20240620\", \"\")\n",
    "    language_model_id = language_model_id.replace(\"claude-3-5-sonnet\", \"claude-3.5-sonnet\")\n",
    "    return language_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\caption{Few Shot Prompting with different language models (n=65)}\n",
      "\\label{tab:few-shot-prompting}\n",
      "\\begin{tabular}{lllrr}\n",
      "\\toprule\n",
      "LM & Attempts & \\begin{tabular}[c]{@{}l@{}}Test\\\\Success\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Test\\\\Errors\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Other\\\\Errors\\end{tabular} \\\\\n",
      "\\midrule\n",
      "claude-3.5-sonnet & 65 & 12 & 28 & 25 \\\\\n",
      "gemini-1.5-pro & 65 & 10 & 24 & 31 \\\\\n",
      "gpt-4o & 65 & 8 & 26 & 30 \\\\\n",
      "claude-3-haiku & 65 & 7 & 22 & 36 \\\\\n",
      "Llama-3.1-70B & 65 & 4 & 7 & 54 \\\\\n",
      "gpt-4o-mini & 65 & 4 & 11 & 50 \\\\\n",
      "mistral-nemo & 65 & 3 & 14 & 48 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "include_full = False\n",
    "\n",
    "def get_language_models():\n",
    "    data_path = Path(os.path.abspath(\"\")) / \"dataset\"\n",
    "    language_models = set()\n",
    "    if include_full:\n",
    "        for path in data_path.rglob(\"out/*-execution-error*\"):\n",
    "            language_models.add(path.parts[-1])\n",
    "    else:\n",
    "        for path in data_path.rglob(\"out/dspy-baseline-*-execution-errors.json\"):\n",
    "            language_models.add(path.parts[-1])\n",
    "\n",
    "    # print(language_models)\n",
    "\n",
    "    return list(language_models)\n",
    "\n",
    "\n",
    "\n",
    "def process_language_model(language_model_id):\n",
    "    data_path = Path(os.path.abspath(\"\")) / \"dataset\"\n",
    "    execution_details = {}\n",
    "    total_error_count = 0\n",
    "    total_compiled = 0\n",
    "    total_full_successes = 0\n",
    "    total_attempts = 0\n",
    "\n",
    "    for path in data_path.rglob(f\"out/{language_model_id}*\"):\n",
    "        commit_hash = path.parts[-3]\n",
    "        total_attempts += 1\n",
    "\n",
    "        pretty_language_model_id = raw_model_mapper(language_model_id)\n",
    "        if include_full:\n",
    "            if \"dspy-baseline-claude-3-haiku\" == pretty_language_model_id:\n",
    "                pretty_language_model_id = \"claude-3-haiku-baseline\"\n",
    "            elif \"full\" in pretty_language_model_id:\n",
    "                pretty_language_model_id = pretty_language_model_id.replace(\"full-\", \"\")\n",
    "                if not re.search(r\"-execution-errors-(\\d+)\", pretty_language_model_id):\n",
    "                    pretty_language_model_id += \"-trial-1\"\n",
    "                else:\n",
    "                    addition= 1\n",
    "                    if \"supplement-\" in pretty_language_model_id:\n",
    "                        addition = 0\n",
    "                    pretty_language_model_id = re.sub(r\"-execution-errors-(\\d+)\", lambda m: f\"-trial-{int(m.group(1)) + addition}\", pretty_language_model_id)\n",
    "                pretty_language_model_id = pretty_language_model_id.replace(\"supplement-\", \"\")\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        if \"dspy-baseline-\" in pretty_language_model_id and not include_full:\n",
    "            pretty_language_model_id = pretty_language_model_id.replace(\"dspy-baseline-\", \"\")\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            execution_errors = json.load(f)\n",
    "            execution_errors = execution_errors.get(commit_hash, execution_errors)\n",
    "            \n",
    "            if len(execution_errors.keys()) == 0:\n",
    "                continue\n",
    "            \n",
    "            if commit_hash not in execution_details:\n",
    "                execution_details[commit_hash] = []\n",
    "            execution_details[commit_hash].append(execution_errors)\n",
    "\n",
    "            full_success = \"tests\" in execution_errors and execution_errors['tests'] == True\n",
    "\n",
    "            \n",
    "\n",
    "            if \"compiled\" in execution_errors and execution_errors[\"compiled\"] == True and not full_success:\n",
    "                total_compiled += 1\n",
    "\n",
    "            elif full_success:\n",
    "                total_full_successes += 1\n",
    "\n",
    "            elif \"error\" in execution_errors and execution_errors['error'] is not None:\n",
    "                total_error_count += 1\n",
    "    \n",
    "    # if total_attempts != 65:\n",
    "    #     return None\n",
    "    return {\n",
    "        \"LM\": pretty_language_model_id,\n",
    "        \"Attempts\": total_attempts,\n",
    "        \"Test Success\": total_full_successes,\n",
    "        \"Test Errors\": total_compiled,\n",
    "        \"Other Errors\": total_error_count,\n",
    "    }\n",
    "\n",
    "def split_header_for_latex(header_name: str): \n",
    "    split = header_name.split(\" \")\n",
    "    if len(split) == 1:\n",
    "        return header_name\n",
    "    return '\\\\begin{tabular}[c]{@{}l@{}}' + '\\\\\\\\'.join(split) + '\\\\end{tabular}'\n",
    "\n",
    "def format_percentage(value, total):\n",
    "    if total == 0:\n",
    "        return f\"{value} (0.0%)\"\n",
    "    percentage = (value / total) * 100\n",
    "    return f\"{value} ({percentage:.1f}\\\\%)\"\n",
    "\n",
    "\n",
    "language_models = get_language_models()\n",
    "data = [process_language_model(lm) for lm in language_models if \"bison\" not in lm]\n",
    "data = [d for d in data if d is not None]\n",
    "\n",
    "assert len(data) > 0, \"No data found\"\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "if not include_full:\n",
    "    df = df.sort_values(['Test Success', \"LM\"], ascending=[False,True])\n",
    "else:\n",
    "    df = df.sort_values(['LM'], ascending=[True])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regular_attempts = df['Attempts'].max()\n",
    "\n",
    "\n",
    "\n",
    "success_df = df.copy()\n",
    "# df['Errors'] = df.apply(lambda row: format_percentage(row['Errors'], row['Attempts']), axis=1)\n",
    "if include_full:\n",
    "    df = df.drop('Attempts', axis=1)\n",
    "    df = df.rename(columns={\"LM\": \"LM Trials\"})\n",
    "latex_columns = [split_header_for_latex(col) for col in df.columns]\n",
    "df.columns = latex_columns\n",
    "\n",
    "\n",
    "   \n",
    "latex_table = df.to_latex(index=False, \n",
    "                            escape=False, \n",
    "                            column_format='lllrr',\n",
    "                            caption=f\"Few Shot Prompting with different language models (n={regular_attempts})\",\n",
    "                            position=\"H\",\n",
    "                            label=\"tab:few-shot-prompting\")\n",
    "    \n",
    "    \n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Time Spent Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:37<04:21, 37.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: gpt-4o\n",
      "Minimum start time: 2024-08-10T11:58:46.061685+00:00\n",
      "Maximum end time: 2024-08-13T01:11:31.050630+00:00\n",
      "Total duration: 2 days, 13:12:44.988945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:55<02:38, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: claude-3-5-sonnet@20240620\n",
      "Minimum start time: 2024-08-15T15:33:58.174027+00:00\n",
      "Maximum end time: 2024-08-16T04:11:24.333630+00:00\n",
      "Total duration: 12:37:26.159603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [01:09<01:41, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: open-mistral-nemo\n",
      "Minimum start time: 2024-08-02T09:56:20.759045+00:00\n",
      "Maximum end time: 2024-08-02T14:57:02.718129+00:00\n",
      "Total duration: 5:00:41.959084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [05:17<07:21, 110.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: gpt-4o-mini\n",
      "Minimum start time: 2024-08-01T20:49:30.989272+00:00\n",
      "Maximum end time: 2024-08-02T09:28:41.812181+00:00\n",
      "Total duration: 12:39:10.822909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [05:22<03:36, 72.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: text-bison@002\n",
      "Minimum start time: 2024-08-05T11:57:47.277504+00:00\n",
      "Maximum end time: 2024-08-05T14:15:10.512269+00:00\n",
      "Total duration: 2:17:23.234765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [05:44<01:50, 55.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: claude-3-haiku@20240307\n",
      "Minimum start time: 2024-08-06T15:06:11.403519+00:00\n",
      "Maximum end time: 2024-08-07T04:12:42.930302+00:00\n",
      "Total duration: 13:06:31.526783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [06:23<00:50, 50.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: gemini-1.5-pro-001\n",
      "Minimum start time: 2024-08-16T18:58:36.547977+00:00\n",
      "Maximum end time: 2024-08-24T08:12:48.396203+00:00\n",
      "Total duration: 7 days, 13:14:11.848226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [06:32<00:00, 49.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model: meta-llama_Meta-Llama-3.1-70B-Instruct-Turbo\n",
      "Minimum start time: 2024-08-12T13:43:30.890825+00:00\n",
      "Maximum end time: 2024-08-12T15:50:00.757549+00:00\n",
      "Total duration: 2:06:29.866724\n",
      "--------------------------------------------------\n",
      "{\"gpt-4o\": {\"input_tokens\": 31239943, \"output_tokens\": 615842}, \"claude-3-5-sonnet@20240620\": {\"input_tokens\": 38146081, \"output_tokens\": 956140}, \"open-mistral-nemo\": {\"input_tokens\": 15409072, \"output_tokens\": 632242}, \"gpt-4o-mini\": {\"input_tokens\": 20910559, \"output_tokens\": 1623186}, \"claude-3-haiku@20240307\": {\"input_tokens\": 32364313, \"output_tokens\": 977883}, \"null\": {\"input_tokens\": 5424, \"output_tokens\": 3}, \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\": {\"input_tokens\": 10116424, \"output_tokens\": 109352}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "token_pattern = r\"Usage\\(input_tokens=(\\d+), output_tokens=(\\d+)\\)\"\n",
    "\n",
    "# usage=UsageInfo(prompt_tokens=2140, total_tokens=2294, completion_tokens=154)\n",
    "mistral_pattern = r\"UsageInfo\\(prompt_tokens=(\\d+), total_tokens=(\\d+), completion_tokens=(\\d+)\\)\"\n",
    "\n",
    "# Initializing defaultdict for token sums\n",
    "token_sums = defaultdict(lambda: {'input_tokens': 0, 'output_tokens': 0})\n",
    "\n",
    "def process_large_jsonl(file_path):\n",
    "    min_start_time = None\n",
    "    max_end_time = None\n",
    "\n",
    "\n",
    "    # {    \"name\": \"log_usage\",    \"context\": {        \"trace_id\": \"0x38aa026246e37c16aefb2492f7c60b07\",        \"span_id\": \"0xccacef7a95f9840e\",        \"trace_state\": \"[]\"    },    \"kind\": \"SpanKind.INTERNAL\",    \"parent_id\": \"0xc831f4697ab76a71\",    \"start_time\": \"2024-08-12T13:04:51.993642Z\",    \"end_time\": \"2024-08-12T13:04:51.993748Z\",    \"status\": {        \"status_code\": \"UNSET\"    },    \"attributes\": {        \"usage\": \"{\\\"prompt_tokens\\\": 12495, \\\"total_tokens\\\": 12498, \\\"completion_tokens\\\": 3}\"    },    \"events\": [],    \"links\": [],    \"resource\": {        \"attributes\": {},        \"schema_url\": \"\"    }}\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        parser = ijson.parse(file, multiple_values=True)\n",
    "        current_model = None  # To track the current model name\n",
    "        in_model_request = False  # Track if we are inside a model request\n",
    "\n",
    "        for prefix, event, value in parser:\n",
    "            # Handle start_time and end_time globally (independent of model requests)\n",
    "            if prefix.endswith('start_time') and event == 'string':\n",
    "                current_start = datetime.fromisoformat(value.rstrip('Z')).replace(tzinfo=timezone.utc)\n",
    "                if min_start_time is None or current_start < min_start_time:\n",
    "                    min_start_time = current_start\n",
    "            elif prefix.endswith('end_time') and event == 'string':\n",
    "                current_end = datetime.fromisoformat(value.rstrip('Z')).replace(tzinfo=timezone.utc)\n",
    "                if max_end_time is None or current_end > max_end_time:\n",
    "                    max_end_time = current_end\n",
    "\n",
    "            # Detect the model name and track if we're inside a model request\n",
    "            if prefix.endswith('llm.model_name') and event == 'string':\n",
    "                current_model = value\n",
    "                in_model_request = True  # Enter model request\n",
    "            elif prefix.endswith('name') and event == 'string':\n",
    "                in_model_request = False  # Exit model request if name field changes\n",
    "\n",
    "            # Process only if inside a model request\n",
    "            if in_model_request and current_model:\n",
    "                # Special handling for Claude models using regex\n",
    "                if current_model.startswith('claude'):\n",
    "                    if prefix.endswith('output.value') and event == 'string':\n",
    "                        match = re.search(token_pattern, value)\n",
    "                        if match:\n",
    "                            token_sums[current_model]['input_tokens'] += int(match.group(1))\n",
    "                            token_sums[current_model]['output_tokens'] += int(match.group(2))\n",
    "                if current_model.startswith('open-mistral'):\n",
    "                    if prefix.endswith('output.value') and event == 'string':\n",
    "                        match = re.search(mistral_pattern, value)\n",
    "                        if match:\n",
    "                            token_sums[current_model]['input_tokens'] += int(match.group(1))\n",
    "                            token_sums[current_model]['output_tokens'] += int(match.group(3))\n",
    "                if current_model.startswith('gpt') or current_model.startswith('meta_llama'): \n",
    "                    if prefix.endswith('output.value') and event == 'string':\n",
    "                        output_data = json.loads(value)\n",
    "                        usage_data = output_data.get('usage', {\"usage\": {\"prompt_tokens\": 0, \"completion_tokens\": 0}})\n",
    "                        token_sums[current_model]['input_tokens'] += usage_data.get('prompt_tokens', 0)\n",
    "                        token_sums[current_model]['output_tokens'] += usage_data.get('completion_tokens', 0)\n",
    "                    \n",
    "\n",
    "            # Handle non-Claude models, assuming attributes.usage contains token data\n",
    "            if prefix.endswith('attributes.usage') and event == 'string':\n",
    "                usage_data = json.loads(value)\n",
    "                token_sums[current_model]['input_tokens'] += usage_data.get('prompt_tokens', 0)\n",
    "                token_sums[current_model]['output_tokens'] += usage_data.get('completion_tokens', 0)\n",
    "\n",
    "    if min_start_time and max_end_time:\n",
    "        duration = max_end_time - min_start_time\n",
    "        return min_start_time, max_end_time, duration\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# glob *_augmented_prompt.json  \n",
    "glob_paths = glob(os.path.join((os.path.abspath(\"\")), \"trace_dspy-baseline*.json\"))\n",
    "\n",
    "# glob_paths = glob(os.path.join((os.path.abspath(\"\")), \"trace_dspy-baseline_meta*.json\"))\n",
    "\n",
    "duration_by_language_model = defaultdict(dict)\n",
    "\n",
    "for file_path in tqdm(glob_paths):\n",
    "    language_model_id = Path(file_path).parts[-1].replace(\"trace_dspy-baseline_\", \"\").replace(\".json\", \"\")\n",
    "\n",
    "    # Usage\n",
    "\n",
    "\n",
    "    min_start, max_end, total_duration = process_large_jsonl(file_path)\n",
    "\n",
    "    duration_by_language_model[language_model_id] = {\"min_start\": min_start, \"max_end\":max_end}\n",
    "\n",
    "\n",
    "\n",
    "    if min_start and max_end:\n",
    "        print(f\"Language Model: {language_model_id}\")\n",
    "        print(f\"Minimum start time: {min_start.isoformat()}\")\n",
    "        print(f\"Maximum end time: {max_end.isoformat()}\")\n",
    "        print(f\"Total duration: {total_duration}\")\n",
    "    else:\n",
    "        print(\"No valid time entries found in the file.\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(json.dumps(token_sums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Summary for haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing and bucketing JSON files: 100%|██████████| 2/2 [04:39<00:00, 139.69s/it]\n",
      "Processing trials: 100%|██████████| 10/10 [00:00<00:00, 154.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import ijson\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "merged_study_name = \"pipeline_claude-3-haiku@20240307_full-supplement\"\n",
    "\n",
    "lm_haiku = \"pipeline_claude-3-haiku@20240307\"\n",
    "storage_name = f\"sqlite:///{lm_haiku}.db\"\n",
    "\n",
    "def parse_usage(value, model):\n",
    "    token_pattern = r\"Usage\\(input_tokens=(\\d+), output_tokens=(\\d+)\\)\"\n",
    "    mistral_pattern = r\"UsageInfo\\(prompt_tokens=(\\d+), total_tokens=(\\d+), completion_tokens=(\\d+)\\)\"\n",
    "    \n",
    "    if model.startswith('claude'):\n",
    "        match = re.search(token_pattern, value)\n",
    "        if match:\n",
    "            return int(match.group(1)), int(match.group(2))\n",
    "    elif model.startswith('open-mistral'):\n",
    "        match = re.search(mistral_pattern, value)\n",
    "        if match:\n",
    "            return int(match.group(1)), int(match.group(3))\n",
    "    elif model.startswith('gpt') or model.startswith('meta_llama'):\n",
    "        try:\n",
    "            output_data = json.loads(value)\n",
    "            usage_data = output_data.get('usage', {})\n",
    "            return usage_data.get('prompt_tokens', 0), usage_data.get('completion_tokens', 0)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    return 0, 0\n",
    "\n",
    "# Create DataFrame with trial information\n",
    "trial_data = []\n",
    "study_summaries = optuna.get_all_study_summaries(storage=storage_name)\n",
    "for summary in study_summaries:\n",
    "    if summary.study_name != merged_study_name:\n",
    "        continue\n",
    "\n",
    "    study = optuna.load_study(study_name=summary.study_name, storage=storage_name)\n",
    "\n",
    "\n",
    "\n",
    "    for trial in study.get_trials():\n",
    "\n",
    "        if trial.datetime_start:\n",
    "            if not trial.datetime_complete:\n",
    "                datetime_complete = datetime.now()\n",
    "            else:\n",
    "                datetime_complete = trial.datetime_complete\n",
    "            trial_data.append({\n",
    "                'study_name': summary.study_name.replace(lm_haiku + \"_\", \"\"),\n",
    "                'trial_number': trial.number,\n",
    "                'start_time': trial.datetime_start.replace(tzinfo=timezone.utc),\n",
    "                'end_time': datetime_complete.replace(tzinfo=timezone.utc),\n",
    "                'duration_hours': (datetime_complete - trial.datetime_start).total_seconds() / 3600,\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'models': set()\n",
    "            })\n",
    "\n",
    "trial_df = pd.DataFrame(trial_data)\n",
    "trial_df = trial_df.sort_values('start_time')\n",
    "\n",
    "# Create a list of (start_time, end_time, index) tuples for efficient bucketing\n",
    "trial_intervals = [(row['start_time'], row['end_time'], idx) for idx, row in trial_df.iterrows()]\n",
    "\n",
    "def bucket_data(item, trial_intervals):\n",
    "    start_time, end_time, model, input_tokens, output_tokens = item\n",
    "    buckets = []\n",
    "    for trial_start, trial_end, trial_idx in trial_intervals:\n",
    "        if (trial_start <= start_time < trial_end) or (trial_start < end_time <= trial_end) or (start_time <= trial_start < end_time):\n",
    "            buckets.append(trial_idx)\n",
    "        # else:\n",
    "            # print(f\"Item {item} does not fit in trial interval {trial_start} - {trial_end}\")\n",
    "    # if len(buckets) > 0:\n",
    "    #     print(f\"Item {item} fits in buckets {buckets}\")\n",
    "    return buckets\n",
    "\n",
    "def process_json_file(file_path, trial_intervals):\n",
    "    bucketed_data = defaultdict(list)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        parser = ijson.parse(file, multiple_values=True)\n",
    "        current_start = None\n",
    "        current_end = None\n",
    "        current_model = None\n",
    "        in_model_request = False\n",
    "        input_tokens = 0\n",
    "        output_tokens = 0\n",
    "\n",
    "        for prefix, event, value in parser:\n",
    "            if prefix.endswith('start_time') and event == 'string':\n",
    "                current_start = datetime.fromisoformat(value.rstrip('Z')).replace(tzinfo=timezone.utc)\n",
    "            elif prefix.endswith('end_time') and event == 'string':\n",
    "                current_end = datetime.fromisoformat(value.rstrip('Z')).replace(tzinfo=timezone.utc)\n",
    "            \n",
    "            if prefix.endswith('llm.model_name') and event == 'string':\n",
    "                current_model = value\n",
    "                in_model_request = True\n",
    "            elif prefix.endswith('name') and event == 'string':\n",
    "                in_model_request = False\n",
    "\n",
    "            if in_model_request and current_model:\n",
    "                if prefix.endswith('output.value') and event == 'string':\n",
    "                    input_tokens, output_tokens = parse_usage(value, current_model)\n",
    "\n",
    "            if prefix.endswith('attributes.usage') and event == 'string':\n",
    "                usage_data = json.loads(value)\n",
    "                input_tokens += usage_data.get('prompt_tokens', 0)\n",
    "                output_tokens += usage_data.get('completion_tokens', 0)\n",
    "\n",
    "            # If we have all the necessary information, process and bucket the item\n",
    "            if current_start and current_end and current_model and (input_tokens > 0 or output_tokens > 0):\n",
    "                item = (current_start, current_end, current_model, input_tokens, output_tokens)\n",
    "                for bucket in bucket_data(item, trial_intervals):\n",
    "                    bucketed_data[bucket].append(item)\n",
    "                \n",
    "                # Reset for the next item\n",
    "                current_start = None\n",
    "                current_end = None\n",
    "                current_model = None\n",
    "                input_tokens = 0\n",
    "                output_tokens = 0\n",
    "                in_model_request = False\n",
    "\n",
    "    return bucketed_data\n",
    "\n",
    "# Process all JSON files\n",
    "all_bucketed_data = defaultdict(list)\n",
    "glob_paths = glob(os.path.join(os.path.abspath(\"\"), \"trace_full*.json\"))\n",
    "for file_path in tqdm(glob_paths, desc=\"Processing and bucketing JSON files\"):\n",
    "    file_data = process_json_file(file_path, trial_intervals)\n",
    "    for bucket, items in file_data.items():\n",
    "        all_bucketed_data[bucket].extend(items)\n",
    "\n",
    "# Function to process bucketed data for a trial\n",
    "def process_trial_bucket(trial_row, bucket_data):\n",
    "    trial_start = trial_row['start_time']\n",
    "    trial_end = trial_row['end_time']\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    models = set()\n",
    "\n",
    "    for start, end, model, in_tokens, out_tokens in bucket_data:\n",
    "        overlap_start = max(trial_start, start)\n",
    "        overlap_end = min(trial_end, end)\n",
    "        overlap_ratio = (overlap_end - overlap_start).total_seconds() / (end - start).total_seconds()\n",
    "        input_tokens += int(in_tokens * overlap_ratio)\n",
    "        output_tokens += int(out_tokens * overlap_ratio)\n",
    "        models.add(model)\n",
    "\n",
    "    return input_tokens, output_tokens, models\n",
    "\n",
    "# Update trial data\n",
    "for idx, row in tqdm(trial_df.iterrows(), total=len(trial_df), desc=\"Processing trials\"):\n",
    "    input_tokens, output_tokens, models = process_trial_bucket(row, all_bucketed_data[idx])\n",
    "    trial_df.at[idx, 'input_tokens'] = input_tokens\n",
    "    trial_df.at[idx, 'output_tokens'] = output_tokens\n",
    "    trial_df.at[idx, 'models'] = models\n",
    "\n",
    "# Convert set of models to string\n",
    "trial_df['models'] = trial_df['models'].apply(lambda x: ', '.join(sorted(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\caption{Parameter importances and trial values}\n",
      "\\label{tab:param\\_importances\\_and\\_trials}\n",
      "\\begin{tabular}{llcccc}\n",
      "\\toprule\n",
      "Parameter & Importance & Baseline & Trial 1 & Trial 2 & Trial 3 & Trial 4 & Trial 5 & Trial 6 & Trial 7 & Trial 8 & Trial 9 \\\\\n",
      "\\midrule\n",
      "API Change & 0.610320 & \\multicolumn{4}{c}{REVAPI} & REVAPI & OMIT & \\multicolumn{4}{c}{REVAPI} \\\\\n",
      "Mvn Error & 0.209128 & MINIFIED & MINIFIED & SUPER\\_MINIFIED & MINIFIED & MINIFIED & MINIFIED & MINIFIED & MINIFIED & OMIT & MINIFIED \\\\\n",
      "Code & 0.064068 & ALL & ALL & ALL & MINIFIED & ALL & ALL & ALL & MINIFIED & ALL & ALL \\\\\n",
      "Max. Hops & 0.062674 & 30 & 30 & 30 & 30 & 40 & 30 & 30 & 30 & 10 & 30 \\\\\n",
      "Dep. Change & 0.034661 & \\multicolumn{4}{c}{MINIFIED\\_PARSED} & MINIFIED\\_PARSED & MINIFIED\\_PARSED & DIFF & OMIT & DIFF & MINIFIED\\_PARSED \\\\\n",
      "LSP Check & 0.019149 & False & True & False & False & False & False & False & True & False & True \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_study_name = \"pipeline_claude-3-haiku@20240307_full-supplement\"\n",
    "\n",
    "merged_study = optuna.load_study(study_name=merged_study_name, storage=storage_name)\n",
    "# param_importances = optuna.importance.get_param_importances(merged_study, evaluator=optuna.importance.PedAnovaImportanceEvaluator())\n",
    "param_importances = optuna.importance.get_param_importances(merged_study)\n",
    "\n",
    "\n",
    "# Create DataFrame for parameter importances\n",
    "df_importances = pd.DataFrame(list(param_importances.items()), columns=['Parameter', 'Importance'])\n",
    "\n",
    "# Extract trial data for each parameter\n",
    "trial_data = {f\"Trial {i}\" if i > 0 else \"Baseline\": trial.params for i, trial in enumerate(merged_study.trials)}\n",
    "\n",
    "# Create a DataFrame from the trial data\n",
    "df_trials = pd.DataFrame(trial_data)\n",
    "\n",
    "# Now, transpose the trial data DataFrame so that parameters are the rows and trials are columns\n",
    "df_trials_transposed = df_trials.T\n",
    "\n",
    "# Merge the importance DataFrame with the transposed trial data\n",
    "df_merged = df_importances.set_index('Parameter').join(df_trials_transposed.T)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "# print(df_merged)\n",
    "\n",
    "\n",
    "latex_table_with_index = df_merged.reset_index().to_latex(index=False, float_format=\"%.6f\", column_format=\"llcccc\", longtable=False, caption=\"Parameter importances and trial values\", label=\"tab:param_importances_and_trials\", position=\"H\")\n",
    "\n",
    "latex_table_with_index = latex_table_with_index.replace(\"_\", \"\\\\_\")\n",
    "\n",
    "\n",
    "latex_table_with_index = latex_table_with_index.replace(\"lspCheck\", \"LSP Check\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"codeType\", \"Code\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"errorType\", \"Mvn Error\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"apiChangeType\",\"API Change\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"dependencyChangeType\", \"Dep. Change\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"max\\\\_hops\", \"Max. Hops\")\n",
    "\n",
    "latex_table_with_index = latex_table_with_index.replace(\"MINIFIED\\\\_PARSED & MINIFIED\\\\_PARSED & MINIFIED\\\\_PARSED & MINIFIED\\\\_PARSED\", \"\\\\multicolumn{4}{c}{MINIFIED\\\\_PARSED}\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"REVAPI & REVAPI & REVAPI & REVAPI\", \"\\\\multicolumn{4}{c}{REVAPI}\")\n",
    "latex_table_with_index = latex_table_with_index.replace(\"0.000000\", \"0\")\n",
    "\n",
    "\n",
    "# Display the LaTeX table\n",
    "print(latex_table_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_name</th>\n",
       "      <th>trial_number</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>duration_hours</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-06 15:06:11.324844+00:00</td>\n",
       "      <td>2024-08-07 04:12:42.947245+00:00</td>\n",
       "      <td>13.108784</td>\n",
       "      <td>32.364M</td>\n",
       "      <td>977.9K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-08-13 11:39:42.897107+00:00</td>\n",
       "      <td>2024-08-14 19:54:09.881340+00:00</td>\n",
       "      <td>32.240829</td>\n",
       "      <td>30.355M</td>\n",
       "      <td>953.4K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-14 19:54:09.968593+00:00</td>\n",
       "      <td>2024-08-15 04:28:21.591849+00:00</td>\n",
       "      <td>8.569895</td>\n",
       "      <td>32.184M</td>\n",
       "      <td>1.149M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-08-15 04:28:21.655739+00:00</td>\n",
       "      <td>2024-08-15 11:16:08.628274+00:00</td>\n",
       "      <td>6.796381</td>\n",
       "      <td>25.808M</td>\n",
       "      <td>1.153M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-08-19 12:13:05.196101+00:00</td>\n",
       "      <td>2024-08-20 01:10:28.716932+00:00</td>\n",
       "      <td>12.956534</td>\n",
       "      <td>43.068M</td>\n",
       "      <td>1.277M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>5</td>\n",
       "      <td>2024-08-20 01:10:28.797313+00:00</td>\n",
       "      <td>2024-08-20 09:16:17.257809+00:00</td>\n",
       "      <td>8.096795</td>\n",
       "      <td>27.891M</td>\n",
       "      <td>1.485M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>6</td>\n",
       "      <td>2024-08-20 09:16:17.308355+00:00</td>\n",
       "      <td>2024-08-20 19:24:21.644748+00:00</td>\n",
       "      <td>10.134538</td>\n",
       "      <td>41.004M</td>\n",
       "      <td>1.432M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-08-20 19:24:21.719026+00:00</td>\n",
       "      <td>2024-08-22 04:26:44.347626+00:00</td>\n",
       "      <td>33.039619</td>\n",
       "      <td>23.255M</td>\n",
       "      <td>1.086M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>8</td>\n",
       "      <td>2024-08-22 04:26:44.405607+00:00</td>\n",
       "      <td>2024-08-22 09:47:19.490838+00:00</td>\n",
       "      <td>5.343079</td>\n",
       "      <td>10.179M</td>\n",
       "      <td>434.1K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>full-supplement</td>\n",
       "      <td>9</td>\n",
       "      <td>2024-08-22 09:47:19.545838+00:00</td>\n",
       "      <td>2024-08-22 12:11:14.601729+00:00</td>\n",
       "      <td>2.398627</td>\n",
       "      <td>3.789M</td>\n",
       "      <td>53.6K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        study_name  trial_number                       start_time  \\\n",
       "0  full-supplement             0 2024-08-06 15:06:11.324844+00:00   \n",
       "1  full-supplement             1 2024-08-13 11:39:42.897107+00:00   \n",
       "2  full-supplement             2 2024-08-14 19:54:09.968593+00:00   \n",
       "3  full-supplement             3 2024-08-15 04:28:21.655739+00:00   \n",
       "4  full-supplement             4 2024-08-19 12:13:05.196101+00:00   \n",
       "5  full-supplement             5 2024-08-20 01:10:28.797313+00:00   \n",
       "6  full-supplement             6 2024-08-20 09:16:17.308355+00:00   \n",
       "7  full-supplement             7 2024-08-20 19:24:21.719026+00:00   \n",
       "8  full-supplement             8 2024-08-22 04:26:44.405607+00:00   \n",
       "9  full-supplement             9 2024-08-22 09:47:19.545838+00:00   \n",
       "\n",
       "                          end_time  duration_hours input_tokens output_tokens  \n",
       "0 2024-08-07 04:12:42.947245+00:00       13.108784      32.364M        977.9K  \n",
       "1 2024-08-14 19:54:09.881340+00:00       32.240829      30.355M        953.4K  \n",
       "2 2024-08-15 04:28:21.591849+00:00        8.569895      32.184M        1.149M  \n",
       "3 2024-08-15 11:16:08.628274+00:00        6.796381      25.808M        1.153M  \n",
       "4 2024-08-20 01:10:28.716932+00:00       12.956534      43.068M        1.277M  \n",
       "5 2024-08-20 09:16:17.257809+00:00        8.096795      27.891M        1.485M  \n",
       "6 2024-08-20 19:24:21.644748+00:00       10.134538      41.004M        1.432M  \n",
       "7 2024-08-22 04:26:44.347626+00:00       33.039619      23.255M        1.086M  \n",
       "8 2024-08-22 09:47:19.490838+00:00        5.343079      10.179M        434.1K  \n",
       "9 2024-08-22 12:11:14.601729+00:00        2.398627       3.789M         53.6K  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_number(x: str, round_to_millions=False):\n",
    "    if pd.isna(x) or x == 'nan' or x == '-' or x == '' or x is None:\n",
    "        return '-'\n",
    "    \n",
    "    try:\n",
    "        # Remove commas and convert to float first\n",
    "        num = float(str(x).replace(',', ''))\n",
    "        \n",
    "        if round_to_millions:\n",
    "                \n",
    "            # Round to nearest million and format\n",
    "            millions = round(num / 1_000_000, 3)\n",
    "\n",
    "\n",
    "            if num < 1_000_000:\n",
    "                # Round to nearest thousand and format\n",
    "                thousands = round(num / 1_000, 1)\n",
    "                if thousands == 0:\n",
    "                    return '-'\n",
    "                if thousands.is_integer():\n",
    "                    return f'{int(thousands)}K'\n",
    "                return f'{thousands:.1f}K'\n",
    "\n",
    "            if millions == 0:\n",
    "                return '-'\n",
    "            if millions.is_integer():\n",
    "                return f'{int(millions)}M'\n",
    "            return f'{millions:.3f}M'\n",
    "        \n",
    "        \n",
    "        \n",
    "    except ValueError:\n",
    "        # If conversion fails, return '-'\n",
    "        return '-'\n",
    "\n",
    "trial_df_formatted = trial_df.copy()\n",
    "\n",
    "trial_df_formatted.loc[0,'input_tokens'] = token_sums.get('claude-3-haiku@20240307', {\"input_tokens\": 0})['input_tokens']\n",
    "trial_df_formatted.loc[0,'output_tokens'] = token_sums.get('claude-3-haiku@20240307', {\"output_tokens\": 0})['output_tokens']\n",
    "    \n",
    "trial_df_formatted['input_tokens'] = trial_df_formatted['input_tokens'].apply(lambda x: format_number(x, round_to_millions=True))\n",
    "trial_df_formatted['output_tokens'] = trial_df_formatted['output_tokens'].apply(lambda x: format_number(x, round_to_millions=True)) \n",
    "\n",
    "\n",
    "trial_df_formatted = trial_df_formatted.drop('models', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "trial_df_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LM</th>\n",
       "      <th>min_start</th>\n",
       "      <th>max_end</th>\n",
       "      <th>duration</th>\n",
       "      <th>duration_secs</th>\n",
       "      <th>duration_hours</th>\n",
       "      <th>total_input_tokens</th>\n",
       "      <th>total_output_tokens</th>\n",
       "      <th>total_input_tokens_short</th>\n",
       "      <th>total_output_tokens_short</th>\n",
       "      <th>total_input_tokens_long</th>\n",
       "      <th>total_output_tokens_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>2024-08-10 11:58:46.061685+00:00</td>\n",
       "      <td>2024-08-13 01:11:31.050630+00:00</td>\n",
       "      <td>0 days 17:26:40.373620</td>\n",
       "      <td>62800.373620</td>\n",
       "      <td>17.444548</td>\n",
       "      <td>31239943</td>\n",
       "      <td>615842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-3.5-sonnet</td>\n",
       "      <td>2024-08-15 15:33:58.174027+00:00</td>\n",
       "      <td>2024-08-16 04:11:24.333630+00:00</td>\n",
       "      <td>0 days 12:37:26.159603</td>\n",
       "      <td>45446.159603</td>\n",
       "      <td>12.623933</td>\n",
       "      <td>38146081</td>\n",
       "      <td>956140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-nemo</td>\n",
       "      <td>2024-08-02 09:56:20.759045+00:00</td>\n",
       "      <td>2024-08-02 14:57:02.718129+00:00</td>\n",
       "      <td>0 days 05:00:41.959084</td>\n",
       "      <td>18041.959084</td>\n",
       "      <td>5.011655</td>\n",
       "      <td>26502242</td>\n",
       "      <td>905165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>2024-08-01 20:49:30.989272+00:00</td>\n",
       "      <td>2024-08-02 09:28:41.812181+00:00</td>\n",
       "      <td>0 days 12:39:10.822909</td>\n",
       "      <td>45550.822909</td>\n",
       "      <td>12.653006</td>\n",
       "      <td>20910559</td>\n",
       "      <td>1623186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-bison-002</td>\n",
       "      <td>2024-08-05 11:57:47.277504+00:00</td>\n",
       "      <td>2024-08-05 14:15:10.512269+00:00</td>\n",
       "      <td>0 days 02:17:23.234765</td>\n",
       "      <td>8243.234765</td>\n",
       "      <td>2.289787</td>\n",
       "      <td>20501428</td>\n",
       "      <td>2365535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>claude-3-haiku</td>\n",
       "      <td>2024-08-06 15:06:11.403519+00:00</td>\n",
       "      <td>2024-08-07 04:12:42.930302+00:00</td>\n",
       "      <td>0 days 13:06:31.526783</td>\n",
       "      <td>47191.526783</td>\n",
       "      <td>13.108757</td>\n",
       "      <td>32364313</td>\n",
       "      <td>977883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gemini-1.5-pro</td>\n",
       "      <td>2024-08-23 15:24:04.674398+00:00</td>\n",
       "      <td>2024-08-24 08:12:48.414666+00:00</td>\n",
       "      <td>0 days 16:48:43.740268</td>\n",
       "      <td>60523.740268</td>\n",
       "      <td>16.812150</td>\n",
       "      <td>119489811</td>\n",
       "      <td>1445613</td>\n",
       "      <td>68374823.0</td>\n",
       "      <td>1361327.0</td>\n",
       "      <td>51114988.0</td>\n",
       "      <td>84286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama-3.1-70B</td>\n",
       "      <td>2024-08-12 13:43:30.890825+00:00</td>\n",
       "      <td>2024-08-12 15:50:00.757549+00:00</td>\n",
       "      <td>0 days 02:06:29.866724</td>\n",
       "      <td>7589.866724</td>\n",
       "      <td>2.108296</td>\n",
       "      <td>10116424</td>\n",
       "      <td>109352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LM                        min_start  \\\n",
       "0             gpt-4o 2024-08-10 11:58:46.061685+00:00   \n",
       "1  claude-3.5-sonnet 2024-08-15 15:33:58.174027+00:00   \n",
       "2       mistral-nemo 2024-08-02 09:56:20.759045+00:00   \n",
       "3        gpt-4o-mini 2024-08-01 20:49:30.989272+00:00   \n",
       "4     text-bison-002 2024-08-05 11:57:47.277504+00:00   \n",
       "5     claude-3-haiku 2024-08-06 15:06:11.403519+00:00   \n",
       "6     gemini-1.5-pro 2024-08-23 15:24:04.674398+00:00   \n",
       "7      Llama-3.1-70B 2024-08-12 13:43:30.890825+00:00   \n",
       "\n",
       "                           max_end               duration  duration_secs  \\\n",
       "0 2024-08-13 01:11:31.050630+00:00 0 days 17:26:40.373620   62800.373620   \n",
       "1 2024-08-16 04:11:24.333630+00:00 0 days 12:37:26.159603   45446.159603   \n",
       "2 2024-08-02 14:57:02.718129+00:00 0 days 05:00:41.959084   18041.959084   \n",
       "3 2024-08-02 09:28:41.812181+00:00 0 days 12:39:10.822909   45550.822909   \n",
       "4 2024-08-05 14:15:10.512269+00:00 0 days 02:17:23.234765    8243.234765   \n",
       "5 2024-08-07 04:12:42.930302+00:00 0 days 13:06:31.526783   47191.526783   \n",
       "6 2024-08-24 08:12:48.414666+00:00 0 days 16:48:43.740268   60523.740268   \n",
       "7 2024-08-12 15:50:00.757549+00:00 0 days 02:06:29.866724    7589.866724   \n",
       "\n",
       "   duration_hours  total_input_tokens  total_output_tokens  \\\n",
       "0       17.444548            31239943               615842   \n",
       "1       12.623933            38146081               956140   \n",
       "2        5.011655            26502242               905165   \n",
       "3       12.653006            20910559              1623186   \n",
       "4        2.289787            20501428              2365535   \n",
       "5       13.108757            32364313               977883   \n",
       "6       16.812150           119489811              1445613   \n",
       "7        2.108296            10116424               109352   \n",
       "\n",
       "   total_input_tokens_short  total_output_tokens_short  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "3                       NaN                        NaN   \n",
       "4                       NaN                        NaN   \n",
       "5                       NaN                        NaN   \n",
       "6                68374823.0                  1361327.0   \n",
       "7                       NaN                        NaN   \n",
       "\n",
       "   total_input_tokens_long  total_output_tokens_long  \n",
       "0                      NaN                       NaN  \n",
       "1                      NaN                       NaN  \n",
       "2                      NaN                       NaN  \n",
       "3                      NaN                       NaN  \n",
       "4                      NaN                       NaN  \n",
       "5                      NaN                       NaN  \n",
       "6               51114988.0                   84286.0  \n",
       "7                      NaN                       NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transpose and create DataFrame\n",
    "df = pd.DataFrame(duration_by_language_model).T\n",
    "\n",
    "# Convert columns to datetime\n",
    "df['min_start'] = pd.to_datetime(df['min_start'])\n",
    "df['max_end'] = pd.to_datetime(df['max_end'])\n",
    "\n",
    "\n",
    "df.loc['gemini-1.5-pro-001', 'min_start'] = pd.to_datetime(\"2024-08-23 15:24:04.674398+00:00\")\n",
    "df.loc['gemini-1.5-pro-001', 'max_end'] = pd.to_datetime(\"2024-08-24 08:12:48.414666+00:00\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate duration and add new column\n",
    "df['duration'] = df['max_end'] - df['min_start']\n",
    "\n",
    "import optuna\n",
    "storage_name = \"sqlite:///{}.db\".format(f\"pipeline_gpt-4o\")\n",
    "study_summaries = optuna.get_all_study_summaries(storage=storage_name)\n",
    "start_times = []\n",
    "end_times = []\n",
    "\n",
    "total_duration = pd.Timedelta(0)\n",
    "\n",
    "for summary in study_summaries:\n",
    "    study = optuna.load_study(study_name=summary.study_name, storage=storage_name)\n",
    "    trials = study.get_trials(deepcopy=False)\n",
    "    for trial in trials:\n",
    "        if trial.datetime_start and trial.datetime_complete:\n",
    "            total_duration += trial.datetime_complete - trial.datetime_start\n",
    "\n",
    "# Due to budget overruns, we split the trials in 2\n",
    "df.loc['gpt-4o', 'duration'] = total_duration\n",
    "\n",
    "\n",
    "\n",
    "df['duration_secs'] = df['duration'].dt.total_seconds()\n",
    "df['duration_hours'] = df['duration'].dt.total_seconds() / 3600\n",
    "\n",
    "# replace index by get_language_model\n",
    "df.index = df.index.map(raw_model_mapper)\n",
    "\n",
    "\n",
    "# Give Index the name \"LM\"\n",
    "df.index.name = \"LM\"\n",
    "\n",
    "gemini_i_short = 68_374_823\n",
    "gemini_i_long = 51_114_988\n",
    "\n",
    "gemini_o_short = 1_361_327\n",
    "gemini_o_long = 84_286\n",
    "\n",
    "metadata = [\n",
    "    {\n",
    "        \"LM\":\"mistral-nemo\",\n",
    "        \"total_input_tokens\": 991_625+25_510_617,\n",
    "        \"total_output_tokens\": 8_496+896_669,\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"gpt-4o-mini\",\n",
    "        \"total_input_tokens\": token_sums[\"gpt-4o-mini\"]['input_tokens'],\n",
    "        \"total_output_tokens\": token_sums[\"gpt-4o-mini\"]['output_tokens'],\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"gemini-1.5-pro\",\n",
    "        \"total_input_tokens_short\": gemini_i_short,\n",
    "        \"total_output_tokens_short\": gemini_o_short,\n",
    "        \"total_input_tokens_long\": gemini_i_long,\n",
    "        \"total_output_tokens_long\": gemini_o_long,\n",
    "        \"total_input_tokens\": gemini_i_short + gemini_i_long,\n",
    "        \"total_output_tokens\": gemini_o_short + gemini_o_long,\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"gpt-4o\",\n",
    "        \"total_input_tokens\": token_sums[\"gpt-4o\"]['input_tokens'],\n",
    "        \"total_output_tokens\": token_sums[\"gpt-4o\"]['output_tokens'],\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"text-bison-002\",\n",
    "        \"total_input_tokens\": 20_501_428,\n",
    "        \"total_output_tokens\": 2_365_535,\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"claude-3-haiku\",\n",
    "        \"total_input_tokens\": token_sums[\"claude-3-haiku@20240307\"]['input_tokens'],\n",
    "        \"total_output_tokens\": token_sums[\"claude-3-haiku@20240307\"]['output_tokens'],\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"claude-3.5-sonnet\",\n",
    "        \"total_input_tokens\": token_sums[\"claude-3-5-sonnet@20240620\"]['input_tokens'],\n",
    "        \"total_output_tokens\": token_sums[\"claude-3-5-sonnet@20240620\"]['output_tokens'],\n",
    "    },\n",
    "    {\n",
    "        \"LM\":\"Llama-3.1-70B\",\n",
    "        \"total_input_tokens\": token_sums[\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"]['input_tokens'],\n",
    "        \"total_output_tokens\": token_sums[\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"]['output_tokens'],\n",
    "    }\n",
    "]\n",
    "\n",
    "# merge in \n",
    "df = df.merge(pd.DataFrame(metadata), on=\"LM\")\n",
    "# print(df)\n",
    "\n",
    "# # Data sourced from billing platforms\n",
    "# df.loc['mistral-nemo', 'total_input_tokens'] = 25_510_617\n",
    "# df.loc['mistral-nemo', 'total_output_tokens'] = 896_669\n",
    "\n",
    "# df.loc['gpt-4o-mini', 'total_input_tokens'] = 36_627_622\n",
    "# df.loc['gpt-4o-mini', 'total_output_tokens'] = 1_240_294\n",
    "\n",
    "# df.loc['text-bison-002', 'total_input_tokens'] = 20_501_428\n",
    "# df.loc['text-bison-002', 'total_output_tokens'] = 2_365_535\n",
    "\n",
    "\n",
    "# df.loc['claude-3-haiku', 'total_input_tokens'] = -1\n",
    "# df.loc['claude-3-haiku', 'total_output_tokens'] = -1\n",
    "\n",
    "# df.loc['Llama-3.1-70B', 'total_input_tokens'] = -1\n",
    "# df.loc['Llama-3.1-70B', 'total_output_tokens'] = -1\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'LM', 'min_start', 'max_end', 'duration', 'duration_secs',\n",
      "       'duration_hours', 'total_input_tokens', 'total_output_tokens',\n",
      "       'total_input_tokens_short', 'total_output_tokens_short',\n",
      "       'total_input_tokens_long', 'total_output_tokens_long'],\n",
      "      dtype='object')\n",
      "Index(['index', 'LM', 'min_start', 'max_end', 'duration', 'duration_secs',\n",
      "       'duration_hours', 'total_input_tokens', 'total_output_tokens',\n",
      "       'total_input_tokens_short', 'total_output_tokens_short',\n",
      "       'total_input_tokens_long', 'total_output_tokens_long', 'total_tokens',\n",
      "       'Attempts', 'Errors', 'Compile Success', 'Test Success'],\n",
      "      dtype='object')\n",
      "[\n",
      "    {\n",
      "        \"LM\":\"claude-3.5-sonnet\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":12.6239332231,\n",
      "        \"DSPY Total Tokens\":39102221,\n",
      "        \"DSPY Compile Success\":28,\n",
      "        \"DSPY Test Success\":12,\n",
      "        \"DSPY Errors\":25,\n",
      "        \"Input Tokens DSPY\":38146081,\n",
      "        \"Output Tokens DSPY\":956140,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"gemini-1.5-pro\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":16.8121500744,\n",
      "        \"DSPY Total Tokens\":120935424,\n",
      "        \"DSPY Compile Success\":24,\n",
      "        \"DSPY Test Success\":10,\n",
      "        \"DSPY Errors\":31,\n",
      "        \"Input Tokens DSPY\":119489811,\n",
      "        \"Output Tokens DSPY\":1445613,\n",
      "        \"Input Tokens Short DSPY\":68374823.0,\n",
      "        \"Output Tokens Short DSPY\":1361327.0,\n",
      "        \"Input Tokens Long DSPY\":51114988.0,\n",
      "        \"Output Tokens Long DSPY\":84286.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"gpt-4o\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":17.4445482278,\n",
      "        \"DSPY Total Tokens\":31855785,\n",
      "        \"DSPY Compile Success\":26,\n",
      "        \"DSPY Test Success\":8,\n",
      "        \"DSPY Errors\":30,\n",
      "        \"Input Tokens DSPY\":31239943,\n",
      "        \"Output Tokens DSPY\":615842,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"claude-3-haiku\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":13.1087574397,\n",
      "        \"DSPY Total Tokens\":33342196,\n",
      "        \"DSPY Compile Success\":22,\n",
      "        \"DSPY Test Success\":7,\n",
      "        \"DSPY Errors\":36,\n",
      "        \"Input Tokens DSPY\":32364313,\n",
      "        \"Output Tokens DSPY\":977883,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"text-bison-002\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":2.2897874347,\n",
      "        \"DSPY Total Tokens\":22866963,\n",
      "        \"DSPY Compile Success\":16,\n",
      "        \"DSPY Test Success\":2,\n",
      "        \"DSPY Errors\":47,\n",
      "        \"Input Tokens DSPY\":20501428,\n",
      "        \"Output Tokens DSPY\":2365535,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"mistral-nemo\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":5.0116553011,\n",
      "        \"DSPY Total Tokens\":27407407,\n",
      "        \"DSPY Compile Success\":14,\n",
      "        \"DSPY Test Success\":3,\n",
      "        \"DSPY Errors\":48,\n",
      "        \"Input Tokens DSPY\":26502242,\n",
      "        \"Output Tokens DSPY\":905165,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"gpt-4o-mini\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":12.6530063636,\n",
      "        \"DSPY Total Tokens\":22533745,\n",
      "        \"DSPY Compile Success\":11,\n",
      "        \"DSPY Test Success\":4,\n",
      "        \"DSPY Errors\":50,\n",
      "        \"Input Tokens DSPY\":20910559,\n",
      "        \"Output Tokens DSPY\":1623186,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    },\n",
      "    {\n",
      "        \"LM\":\"Llama-3.1-70B\",\n",
      "        \"Attempts\":65,\n",
      "        \"DSPY Duration (hours)\":2.1082963122,\n",
      "        \"DSPY Total Tokens\":10225776,\n",
      "        \"DSPY Compile Success\":7,\n",
      "        \"DSPY Test Success\":4,\n",
      "        \"DSPY Errors\":54,\n",
      "        \"Input Tokens DSPY\":10116424,\n",
      "        \"Output Tokens DSPY\":109352,\n",
      "        \"Input Tokens Short DSPY\":0.0,\n",
      "        \"Output Tokens Short DSPY\":0.0,\n",
      "        \"Input Tokens Long DSPY\":0.0,\n",
      "        \"Output Tokens Long DSPY\":0.0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "new_df = df.copy()\n",
    "\n",
    "# Move index into a column \"LM\"\n",
    "new_df.reset_index(inplace=True)\n",
    "\n",
    "print(new_df.columns)\n",
    "\n",
    "new_df['total_tokens'] = new_df['total_input_tokens'] + new_df['total_output_tokens']\n",
    "# df['total_tokens'] = df['total_tokens'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# new_df[\"DSPY Compile Success\"] = 0\n",
    "# new_df[\"DSPY Test Success\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "# merged_df = new_df.merge(success_df, left_on='LM', right_on='LM', how='left')\n",
    "merged_df = new_df.merge(success_df, how=\"right\", on=\"LM\")\n",
    "# print(merged_df)\n",
    "# merged_df['DSPY Compile Success'] = (merged_df['Compile Success'] / merged_df['Attempts']).apply(lambda x: f\"{x:.2%}\" if not pd.isna(x) else '-')\n",
    "# merged_df['DSPY Test Success'] = (merged_df['Test Success'] / merged_df['Attempts']).apply(lambda x: f\"{x:.2%}\" if not pd.isna(x) else '-')\n",
    "\n",
    "print(merged_df.columns)\n",
    "\n",
    "merged_df.rename(columns={'Test Success': 'DSPY Test Success', 'Compile Success': 'DSPY Compile Success'}, inplace=True)\n",
    "\n",
    "merged_df['DSPY Errors'] = merged_df['Errors']\n",
    "merged_df['DSPY Attempts'] = merged_df['Attempts']\n",
    "\n",
    "\n",
    "\n",
    "# LaTex Table with LM and duration_hours\n",
    "# final_df = merged_df[['LM', 'duration_hours', 'total_tokens', 'DSPY Compile Success', 'DSPY Test Success']]\n",
    "final_df = merged_df.rename(columns={'duration_hours': 'DSPY Duration (hours)', 'total_tokens': 'DSPY Total Tokens', \n",
    "                                     'total_input_tokens': 'Input Tokens DSPY', 'total_output_tokens': 'Output Tokens DSPY','total_input_tokens_short': 'Input Tokens Short DSPY',\n",
    "'total_output_tokens_short': 'Output Tokens Short DSPY',\n",
    "'total_input_tokens_long': 'Input Tokens Long DSPY',\n",
    "'total_output_tokens_long': 'Output Tokens Long DSPY'})\n",
    "\n",
    "\n",
    "final_df = final_df[[\"LM\",\n",
    "                     \"Attempts\",\n",
    "\"DSPY Duration (hours)\",\n",
    "\"DSPY Total Tokens\",\n",
    "\"DSPY Compile Success\",\n",
    "\"DSPY Test Success\",\n",
    "\"DSPY Errors\",\n",
    "'Input Tokens DSPY',\n",
    "'Output Tokens DSPY',\n",
    "'Input Tokens Short DSPY',\n",
    "'Output Tokens Short DSPY',\n",
    "'Input Tokens Long DSPY',\n",
    "'Output Tokens Long DSPY',\n",
    "                     ]]\n",
    "\n",
    "final_df = final_df.fillna(0)\n",
    "print(final_df.to_json(orient='records', indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = final_df.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.2f\", \n",
    "    caption=\"Duration of Augmented Prompt Execution by Language Model\", \n",
    "    label=\"tab:augmented-prompt-duration\", \n",
    "    position=\"H\"\n",
    ")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the hop value via the hops dict for the commit hashes in compiled-hash and full_successes-hash in trial_data for each trial\n",
    "# \"hops\": {\n",
    "#             \"6c53cd904bd66fc79af8687571e607c259226b81\": 10,\n",
    "#             \"8502e85f9ee2ff90ce96b47b5904f011e81e8bb8\": 10,\n",
    "#             \"489aad6060454d0b7b34a144e0b345c5a3a199f5\": 10,\n",
    "#             \"165381d26b2c3d2278fde88c16f95807506451fe\": 10,\n",
    "#             \"8fbb6deb112102ef7507a8e68c5215e5f481d03b\": 10,\n",
    "#             \"250cafc7d6ae47d5d4803b5a5e58186eb81fa3b5\": 10,\n",
    "#             \"c32185c43be158d32c7d13c5b816991954eb45fa\": 10,\n",
    "#             \"2d733a58045b4bf3669aa00d875e77f9db48c29b\": 10,\n",
    "#             \"dc9f7910968cd0aa2090e390045ae053693e839a\": 10,\n",
    "#             \"a2b0fc53611f8705640773f18c8dd6a47eed3b7f\": 10,\n",
    "#             \"a4c360001134c2e3a9f7fbde88a07a9fd767e78e\": 10,\n",
    "#             \"ad80bdff62b1b0520d3fb9e8d627532a38a7c60c\": 10,\n",
    "#             \"ea33b5101edffc0242967cbf21c1016378b18483\": 10,\n",
    "#             \"1d43bce1de6a81ac017c233d72f348d3c850299e\": 10,\n",
    "#             \"979d6237a50840cd925cc1a33c415ffbbbc42846\": 10,\n",
    "#             \"af6e5d1cc94f031f29b4838e7a8b56704c8c5de4\": 10,\n",
    "#             \"d3af06df4613be146bb9f8034e1a8a3098050c82\": 10,\n",
    "#             \"c7c9590a206d4fb77dd05b9df391d888e6181667\": 10,\n",
    "#             \"14fc5fa696f499cac48401b3a86882b3bf7d9b82\": 10,\n",
    "#             \"b6a48a6e557fad1ceda680618e0a34c7b8c5c087\": 10,\n",
    "#             \"5fcd0c3ad7727850c47602b17530dc355e5bd097\": 10,\n",
    "#             \"3ff575ae202cdf76ddfa8a4228a1711a6fa1e921\": 10,\n",
    "#             \"4b4c08d502d98d240855013ab76008f5e0243435\": 10,\n",
    "#             \"3572a1ecc0154c61e05505aed56055b9c5e539a6\": 10,\n",
    "#             \"acc50dabec6796c091b84c1ada2ae4cbcab8b562\": 10\n",
    "#         },\n",
    "\n",
    "hop_values = []\n",
    "for index, row in trial_data.items():\n",
    "  print(row)\n",
    "  compiled_hashes = row.get('compiled-hash', None)\n",
    "  full_successes_hashes = row.get('full_successes-hash', None)\n",
    "  hops = row['hops']\n",
    "\n",
    "  # print(compiled_hashes, full_successes_hashes)\n",
    "\n",
    "  # for hash in compiled_hashes:\n",
    "  #   print(hops.get(hash))\n",
    "  \n",
    "  compiled_hop = hops.get(compiled_hashes[0]) if compiled_hashes else None\n",
    "  full_successes_hop = hops.get(full_successes_hashes[0]) if full_successes_hashes else None\n",
    "  \n",
    "  hop_values.append((compiled_hop, full_successes_hop))\n",
    "  \n",
    "hop_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(trial_data, orient='index')\n",
    "\n",
    "# Sorting by trial number\n",
    "df_sorted = df.sort_index()\n",
    "df_sorted.index = df_sorted.index.astype(int)\n",
    "df_sorted = df_sorted.sort_index()\n",
    "\n",
    "# Plotting the data as a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotting 'compiled' and 'full_successes' as bar charts\n",
    "df_sorted[['compiled', 'full_successes', 'tests_broken_in_environment', 'no_repro']].plot(kind='bar', ax=ax)\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Compiled and Full Successes by Trial Number {language_model_id}')\n",
    "\n",
    "ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Adding legend\n",
    "ax.legend(['Compiled (total)', 'Test Suite completed', 'Test Suite broken in environment', 'no_repro'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data with 'attempts_not_compiled' always on top as a bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Reordering the columns to have 'attempts_not_compiled' on top\n",
    "df_sorted_reordered = df_sorted[['full_successes', 'tests_broken_in_environment', 'no_repro', 'attempts_not_compiled']]\n",
    "\n",
    "# Plotting the data\n",
    "df_sorted_reordered.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Compilation Attempts and Results (for valid diff attempts by the LLM) by Trial Number for {language_model_id}')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Adding legend\n",
    "ax.legend(['Full Successes', 'Broken Tests', 'No Repro', 'Failed Compilation Attempts'])\n",
    "\n",
    "# Ensure the y-axis only shows full numbers\n",
    "ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "# Adding data labels, only showing numbers if not 0\n",
    "for container in ax.containers:\n",
    "    labels = [f'{v:.0f}' if v != 0 else '' for v in container.datavalues]\n",
    "    ax.bar_label(container, labels=labels, label_type='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sorted = pd.DataFrame.from_dict(trial_data, orient='index')\n",
    "error_details_columns = [col for col in df_sorted.columns if col.startswith('error_details_')]\n",
    "\n",
    "# Plotting the data with error details as a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plotting error details as stacked bar charts\n",
    "df_sorted[error_details_columns].plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'Pre-Compilation Error Details by Trial Number for {language_model_id}')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Adding legend\n",
    "ax.legend([col.replace('error_details_', '').replace('_', ' ').title() for col in error_details_columns])\n",
    "\n",
    "# Ensure the y-axis only shows full numbers\n",
    "ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "# Adding data labels, only showing numbers if not 0\n",
    "for container in ax.containers:\n",
    "    labels = [f'{v:.0f}' if v != 0 else '' for v in container.datavalues]\n",
    "    ax.bar_label(container, labels=labels, label_type='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for error prompt tokens\n",
    "\n",
    "# Research Question: Are errors persistent between the executions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "language_model_id = \"claude-3-haiku@20240307\"\n",
    "trial = \"full_supplement\"\n",
    "study_name = \"pipeline_\" + f\"{language_model_id}_\"+trial\n",
    "print(\"Loading study\", study_name)\n",
    "storage_name = \"sqlite:///{}.db\".format(f\"pipeline_{language_model_id}\")\n",
    "study = optuna.load_study(storage=storage_name, study_name=study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Load the two studies\n",
    "study1 = optuna.load_study(study_name=\"pipeline_claude-3-haiku@20240307_dspy-baseline\", storage=storage_name)\n",
    "study2 = optuna.load_study(study_name=\"pipeline_claude-3-haiku@20240307_full_supplement\", storage=storage_name)\n",
    "\n",
    "\n",
    "merged_study_name = \"pipeline_claude-3-haiku@20240307_merged_study\"\n",
    "\n",
    "if merged_study_name not in optuna.get_all_study_names(storage_name):\n",
    "\n",
    "    # Create a new study to merge them\n",
    "    merged_study = optuna.create_study(study_name=merged_study_name, direction=\"maximize\", storage=storage_name)\n",
    "\n",
    "    # Function to add trials from one study to another\n",
    "    def add_trials_to_study(source_study, target_study):\n",
    "        for trial in source_study.trials:\n",
    "            target_study.add_trial(trial)\n",
    "\n",
    "    # Merge the trials from both studies into the new one\n",
    "    add_trials_to_study(study1, merged_study)\n",
    "    add_trials_to_study(study2, merged_study)\n",
    "\n",
    "else:\n",
    "    # Load the merged study\n",
    "    merged_study = optuna.load_study(study_name=merged_study_name, storage=storage_name)\n",
    "\n",
    "# optuna.study.save_study_to_storage(merged_study)\n",
    "\n",
    "# Get the new parameter importances for the merged study\n",
    "\n",
    "# Display the parameter importances\n",
    "print(param_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "fixed path src/main/java/io/github/hapjava/server/impl/crypto/ChachaDecoder.java\n",
      "fixed path src/main/java/io/github/hapjava/server/impl/crypto/ChachaDecoder.java\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n",
      "Repository is dirty. Discarding changes...\n",
      "All changes have been discarded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>language_model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">files_modified</th>\n",
       "      <th colspan=\"3\" halign=\"left\">operation_count</th>\n",
       "      <th colspan=\"3\" halign=\"left\">hunk_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude-3-5-sonnet@20240620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.585923</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.621582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude-3-haiku@20240307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.142857</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.899735</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.690066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemini-1.5-pro-001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.475730</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.695896</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.517549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.258306</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meta-llama_Meta-Llama-3.1-70B-Instruct-Turbo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open-mistral-nemo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text-bison@002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 language_model files_modified              \\\n",
       "                                                          mean median  std   \n",
       "0                    claude-3-5-sonnet@20240620            1.0    1.0  0.0   \n",
       "1                       claude-3-haiku@20240307            1.0    1.0  0.0   \n",
       "2                            gemini-1.5-pro-001            1.0    1.0  0.0   \n",
       "3                                        gpt-4o            1.0    1.0  0.0   \n",
       "4                                   gpt-4o-mini            1.0    1.0  0.0   \n",
       "5  meta-llama_Meta-Llama-3.1-70B-Instruct-Turbo            1.0    1.0  0.0   \n",
       "6                             open-mistral-nemo            1.0    1.0  0.0   \n",
       "7                                text-bison@002            1.0    1.0  0.0   \n",
       "\n",
       "  operation_count                  hunk_count                   \n",
       "             mean median       std       mean median       std  \n",
       "0        3.833333    4.0  1.585923   1.750000    2.0  0.621582  \n",
       "1        3.142857    3.0  0.899735   1.857143    2.0  0.690066  \n",
       "2        2.800000    2.0  1.475730   1.100000    1.0  0.316228  \n",
       "3        4.125000    3.5  2.695896   1.625000    2.0  0.517549  \n",
       "4        3.750000    4.0  1.258306   2.000000    2.0  0.816497  \n",
       "5        2.250000    2.0  0.500000   1.000000    1.0  0.000000  \n",
       "6        3.333333    3.0  0.577350   1.333333    1.0  0.577350  \n",
       "7        6.000000    6.0  2.828427   1.500000    1.5  0.707107  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "diffs = defaultdict(dict) \n",
    "data_path = Path(os.path.abspath(\"\"))/\"dataset\"\n",
    "import tempfile\n",
    "from unidiff import PatchSet\n",
    "\n",
    "from masterthesis.agent.GitAgent import GitAgent\n",
    "\n",
    "git_slugs = defaultdict(str)\n",
    "\n",
    "for file in data_path.glob(f\"*/out/dspy-baseline-*-execution-errors.json\"):\n",
    "  lm = file.parts[-1].replace(\"-execution-errors.json\", \"\").replace(\"dspy-baseline-\", \"\")\n",
    "  commit = file.parts[-3]\n",
    "  with open(file, \"r\") as f:\n",
    "    execution_errors = json.load(f)\n",
    "    if \"output\" in execution_errors:\n",
    "      if len(execution_errors[\"output\"]) == 1:\n",
    "        diff = execution_errors[\"output\"][0]\n",
    "        # diff = diff.replace('```diff', '').replace('```', '')\n",
    "        if commit not in diffs[lm]:\n",
    "              diffs[lm][commit] = set()\n",
    "        diffs[lm][commit].add(diff)\n",
    "        git_slugs.update({commit: execution_errors[\"input\"][\"repo_slug\"]})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from unidiff import PatchSet\n",
    "\n",
    "from masterthesis.agent.aider.AdvancedDiffAgent import UnifiedDiffCoder\n",
    "\n",
    "# Function to calculate diff statistics\n",
    "def calculate_diff_stats(diff, commit, language_model):\n",
    "\n",
    "    repo_path = data_path / f\"{commit}/repo\"\n",
    "    git_agent = GitAgent(Path(repo_path), commit_hash=commit, github_slug=git_slugs[commit])\n",
    "    git_agent.discard_changes()\n",
    "    coder = UnifiedDiffCoder(repo_dir=repo_path)\n",
    "    for diff_text in [diff]:\n",
    "        paths = coder.get_paths(diff_text)\n",
    "        for path in paths:\n",
    "\n",
    "            output_location = Path(repo_path) / path\n",
    "            is_valid_diff, patched_file = coder.apply_edits(diff_text)\n",
    "            # assert is_valid_diff, f\"Diff for {path} is not valid\"\n",
    "\n",
    "            if is_valid_diff:\n",
    "\n",
    "                with open(output_location, \"w\", encoding=\"utf-8\") as out_file_wrapper:\n",
    "                    out_file_wrapper.write(patched_file)\n",
    "    full_diff = git_agent.get_full_diff()\n",
    "    with open(data_path / commit / f\"{language_model}.diff\", \"w\") as f:\n",
    "        f.write(full_diff)\n",
    "    patch = PatchSet(full_diff)\n",
    "    git_agent.discard_changes()\n",
    "        \n",
    "\n",
    "    \n",
    "    operation_count = sum([file_patch.added+file_patch.removed for file_patch in patch])\n",
    "    hunk_count = sum([len(file_patch) for file_patch in patch])\n",
    "\n",
    "    return {\n",
    "        'files_modified': len(patch),  \n",
    "        'operation_count': operation_count,\n",
    "        'hunk_count': hunk_count\n",
    "    }\n",
    "\n",
    "# Transform diffs and add statistics\n",
    "transformed_diffs = defaultdict(lambda: defaultdict(list))\n",
    "for lm, commits in diffs.items():\n",
    "    for commit, diff_set in commits.items():\n",
    "        for diff in diff_set:\n",
    "            stats = calculate_diff_stats(diff, commit, lm)\n",
    "            transformed_diffs[lm][commit].append({\n",
    "                'diff': diff,\n",
    "                'files_modified': stats['files_modified'],\n",
    "                'operation_count': stats['operation_count'],\n",
    "                'hunk_count': stats['hunk_count']\n",
    "            })\n",
    "\n",
    "# Create a DataFrame\n",
    "rows = []\n",
    "for lm, commits in transformed_diffs.items():\n",
    "    for commit, diffs in commits.items():\n",
    "        for diff_data in diffs:\n",
    "            rows.append({\n",
    "                'language_model': lm,\n",
    "                'commit': commit,\n",
    "                'files_modified': diff_data['files_modified'],\n",
    "                'operation_count': diff_data['operation_count'],\n",
    "                'hunk_count': diff_data['hunk_count']\n",
    "            })\n",
    "\n",
    "diff_df = pd.DataFrame(rows)\n",
    "\n",
    "# Commit-level statistics\n",
    "commit_stats = diff_df.groupby(['language_model', 'commit']).agg({\n",
    "    'files_modified': ['mean', 'median', 'std'],\n",
    "    'operation_count': ['mean', 'median', 'std'],\n",
    "    'hunk_count': ['mean', 'median', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Language model comparison\n",
    "lm_comparison = diff_df.groupby('language_model').agg({\n",
    "    'files_modified': ['mean', 'median', 'std'],\n",
    "    'operation_count': ['mean', 'median', 'std'],\n",
    "    'hunk_count': ['mean', 'median', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# diff_df.to_pickle(\"diff_stats.pkl\")\n",
    "\n",
    "lm_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[H]\n",
      "\\caption{Diff Statistics by Language Model Zero Shot}\n",
      "\\label{tab:diff-stats-zero-shot}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "Language Model & Files Modified & Operation Count & Hunk Count \\\\\n",
      "\\midrule\n",
      "claude-3.5-sonnet & 1 (1) & 3.83 $\\pm$ 1.59 (4) & 1.75 $\\pm$ 0.62 (2) \\\\\n",
      "gemini-1.5-pro & 1 (1) & 2.80 $\\pm$ 1.48 (2) & 1.10 $\\pm$ 0.32 (1) \\\\\n",
      "gpt-4o & 1 (1) & 4.12 $\\pm$ 2.70 (3.5) & 1.62 $\\pm$ 0.52 (2) \\\\\n",
      "claude-3-haiku & 1 (1) & 3.14 $\\pm$ 0.90 (3) & 1.86 $\\pm$ 0.69 (2) \\\\\n",
      "Llama-3.1-70B & 1 (1) & 2.25 $\\pm$ 0.5 (2) & 1 (1) \\\\\n",
      "gpt-4o-mini & 1 (1) & 3.75 $\\pm$ 1.26 (4) & 2 $\\pm$ 0.82 (2) \\\\\n",
      "mistral-nemo & 1 (1) & 3.33 $\\pm$ 0.58 (3) & 1.33 $\\pm$ 0.58 (1) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:58: SyntaxWarning: invalid escape sequence '\\p'\n",
      "/tmp/ipykernel_632508/1536726878.py:58: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  latex_text = latex_text.replace(\"±\", \"$\\pm$\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lm_comparison_human = lm_comparison.rename(columns={'files_modified': 'Files Modified', 'operation_count': 'Operation Count', 'language_model': \"Language Model\"})\n",
    "\n",
    "# print(lm_comparison_human)\n",
    "\n",
    "def convert_float_to_int(value):\n",
    "    # if isinstance(value, float):\n",
    "    #     print(value, value.is_integer())\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return str(int(value))\n",
    "    return value\n",
    "# lm_comparison_human = lm_comparison_human.map(convert_float_to_int)\n",
    "\n",
    "\n",
    "\n",
    "def format_mean_std_median(mean, std, median):\n",
    "    if np.isnan(mean) or np.isnan(std) or np.isnan(median):\n",
    "        return \"N/A\"\n",
    "    \n",
    "    if mean.is_integer():\n",
    "        mean_str = str(int(mean))\n",
    "    else:\n",
    "        mean_str = f\"{mean:.2f}\"\n",
    "    mean_std = f\"{mean_str} ± {std:.2f}\"\n",
    "    if mean.is_integer() and std == 0:\n",
    "        mean_std = f\"{int(mean)}\"\n",
    "    if median.is_integer():\n",
    "        median_str = str(int(median))\n",
    "    else:\n",
    "        median_str = f\"{median:.2f}\"\n",
    "    return f\"{mean_std} ({median_str})\"\n",
    "\n",
    "# Create a new DataFrame for the formatted data\n",
    "formatted_data = []\n",
    "\n",
    "for lm, row in lm_comparison.iterrows():\n",
    "    formatted_row = {\n",
    "        'Language Model': raw_model_mapper(row[\"language_model\"].to_string().strip()),\n",
    "        'Files Modified': format_mean_std_median(row['files_modified']['mean'], row['files_modified']['std'], row['files_modified']['median']),\n",
    "        'Operation Count': format_mean_std_median(row['operation_count']['mean'], row['operation_count']['std'], row['operation_count']['median']),\n",
    "        'Hunk Count': format_mean_std_median(row['hunk_count']['mean'], row['hunk_count']['std'], row['hunk_count']['median'])\n",
    "    }\n",
    "    formatted_data.append(formatted_row)\n",
    "\n",
    "formatted_df = pd.DataFrame(formatted_data)\n",
    "\n",
    "lm_order = success_df['LM'].tolist()\n",
    "formatted_df = formatted_df[formatted_df['Language Model'].isin(lm_order)]\n",
    "formatted_df['LM'] = pd.Categorical(formatted_df['Language Model'], categories=lm_order, ordered=True)\n",
    "formatted_df = formatted_df.sort_values('LM').drop(columns=['LM'])\n",
    "\n",
    "\n",
    "\n",
    "latex_text = formatted_df.to_latex(index=False, escape=False, float_format=\"%.2f\", position=\"H\", caption=\"Diff Statistics by Language Model Zero Shot\", label=\"tab:diff-stats-zero-shot\")\n",
    "                                          # , column_format=\"lrrr|rrr\")\n",
    "\n",
    "latex_text = latex_text.replace(\"±\", \"$\\pm$\")\n",
    "latex_text = latex_text.replace(\".50\", \".5\")\n",
    "\n",
    "\n",
    "\n",
    "print(latex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['489aad6060454d0b7b34a144e0b345c5a3a199f5',\n",
       " 'c7c9590a206d4fb77dd05b9df391d888e6181667',\n",
       " '8502e85f9ee2ff90ce96b47b5904f011e81e8bb8',\n",
       " '90ffd2cd31edecf778d14d0015da9ceab7e53081',\n",
       " '3ff575ae202cdf76ddfa8a4228a1711a6fa1e921',\n",
       " 'f26cd85b97b24c07a2e446f43ac8793619fa0724',\n",
       " '5fcd0c3ad7727850c47602b17530dc355e5bd097',\n",
       " 'ea33b5101edffc0242967cbf21c1016378b18483',\n",
       " 'a80dac86d1caa3958c45c036d93a7d9231d88fbf',\n",
       " '1d43bce1de6a81ac017c233d72f348d3c850299e',\n",
       " '0a11c04038eae517540051dbf51f7f26b7221f20',\n",
       " 'a2b0fc53611f8705640773f18c8dd6a47eed3b7f',\n",
       " '1ef97ea6c5b6e34151fe6167001b69e003449f95',\n",
       " 'd3af06df4613be146bb9f8034e1a8a3098050c82',\n",
       " 'c0f6ab75784dbc13ae8ff47298704c0756cf3a2c',\n",
       " 'dcc95f410847ab308db2f2a31ab13e32dc65c670',\n",
       " 'ae16b526695fe275ab5e6a1992916875d26da860',\n",
       " '65200df71d5f6ab1c5502f74a5dc7bcbda459563',\n",
       " 'cbcafe129e143ef09401470e9d11de9758f298d0',\n",
       " '36859167815292f279e570d39dd2ddbcf1622dc6',\n",
       " '0abf7148300f40a1da0538ab060552bca4a2f1d8',\n",
       " '874ed893a4e46ea5182be2be054715967e58f08f',\n",
       " 'b6a48a6e557fad1ceda680618e0a34c7b8c5c087',\n",
       " '1820a966ae02ad8df44d0a0106cba65ceaf3aa95',\n",
       " '1fc5281e0688c44025fe2b390a7d6e3e3088f385',\n",
       " '2dfaa41bfb97674d11f09a5885011f19808548a3',\n",
       " 'd54b56b91c11f21b97d4903143b04b7c1f10c255',\n",
       " '165381d26b2c3d2278fde88c16f95807506451fe',\n",
       " 'c32185c43be158d32c7d13c5b816991954eb45fa',\n",
       " 'a4c360001134c2e3a9f7fbde88a07a9fd767e78e',\n",
       " '28be199c825d419957bc753a9519e8e9ecc6a08e',\n",
       " 'af6e5d1cc94f031f29b4838e7a8b56704c8c5de4',\n",
       " '8fbb6deb112102ef7507a8e68c5215e5f481d03b',\n",
       " '07fad972bb884e9fa6143b4f870d08305811607d',\n",
       " 'ad80bdff62b1b0520d3fb9e8d627532a38a7c60c',\n",
       " '88a20ece4db960e35fbfa39fcb40e61daceb15b1',\n",
       " 'b8f92ff37d1aed054d8320283fd6d6a492703a55',\n",
       " '43b3a858b77ec27fc8946aba292001c3de465012',\n",
       " '95b2c15de16fd9fd612ce73672e29b613ce7a909',\n",
       " 'f714a41f0ffe9720939d4980da5119d1f45bd770',\n",
       " 'acc50dabec6796c091b84c1ada2ae4cbcab8b562',\n",
       " '0e8625f492854a78c0e1ceff67b2abd7e081d42b',\n",
       " '6c53cd904bd66fc79af8687571e607c259226b81',\n",
       " '3572a1ecc0154c61e05505aed56055b9c5e539a6',\n",
       " 'dbdc7d2c4a28a8d65edcd0cdece91c0bc357b869',\n",
       " '250cafc7d6ae47d5d4803b5a5e58186eb81fa3b5',\n",
       " 'f6659d758a437f8b676481fe70671a68a6ee1cde',\n",
       " '6ad104c4fb9263ad1bb29e6b33618b8225efd92d',\n",
       " '7fb959ccb8c9b32bd6cbc9fc95ed70c4d9c85575',\n",
       " '10d7545c5771b03dd9f6122bd5973a759eb2cd03',\n",
       " '2d733a58045b4bf3669aa00d875e77f9db48c29b',\n",
       " '14fc5fa696f499cac48401b3a86882b3bf7d9b82',\n",
       " '067f5d2c81ff87c90755f4ed48f62eb5faa8ecf9',\n",
       " 'dc9f7910968cd0aa2090e390045ae053693e839a',\n",
       " '5adde4f1309a1078b39d013a30dc392c97ca7543',\n",
       " '979d6237a50840cd925cc1a33c415ffbbbc42846',\n",
       " 'd401e189fb6435110e3dc4ca1a94838f167e7ddf',\n",
       " 'dc9a40fde9a9fee5aaec3f60695385ba539406d4',\n",
       " '5287fc631fa78e7f11d39983824cdd4215b9a03b',\n",
       " 'a26797cdeeecaa3b900ea1e0d5ec0cec66bf03ff',\n",
       " '9461431622cf39efe60cf1eb03a94083780c5720',\n",
       " '7f7de81d28b68b091bef2e6f6ffd1836167be6ea',\n",
       " '3d29f9a6823fa68763d3148bc0353ac557f2a815',\n",
       " '4b4c08d502d98d240855013ab76008f5e0243435',\n",
       " '0305beafdecb0b28f7c94264ed20cdc4e41ff067']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dspy_hashes = []\n",
    "data_path = Path(os.path.abspath(\"\")) / \"dataset\"\n",
    "for file in data_path.glob(\"*\"):\n",
    "    if file.is_dir():\n",
    "        dspy_hashes.append(file.parts[-1])\n",
    "dspy_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import git\n",
    "import tempfile\n",
    "import subprocess\n",
    "import platform\n",
    "import filecmp\n",
    "import stat\n",
    "\n",
    "def clone_repo(github_slug, commit_hash):\n",
    "    with tempfile.TemporaryDirectory(delete=False) as repo_path:\n",
    "        start_time = time.time()\n",
    "        repo = git.Repo.clone_from(f\"https://github.com/{github_slug}.git\", repo_path)\n",
    "        repo.git.fetch(\"origin\", commit_hash, depth=2)\n",
    "        repo.git.checkout(commit_hash)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time, repo_path\n",
    "\n",
    "def copy_folder(source_path, commit_hash):\n",
    "    with tempfile.TemporaryDirectory(delete=False) as dest_path:\n",
    "        start_time = time.time()\n",
    "        shutil.copytree(source_path, dest_path, symlinks=True, ignore=None, dirs_exist_ok=True)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time, dest_path\n",
    "\n",
    "def get_directory_hash(directory):\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        hash_command = [\"find\", directory, \"-type\", \"f\", \"-not\", \"-path\", \"*/\\.*\", \"-print0\", \"|\", \"xargs\", \"-0\", \"shasum\", \"-a\", \"256\", \"|\", \"LC_ALL=C\", \"sort\", \"-k\", \"2\", \"|\", \"shasum\", \"-a\", \"256\"]\n",
    "    else:  # Linux and other Unix-like systems\n",
    "        hash_command = [\"find\", directory, \"-type\", \"f\", \"-not\", \"-path\", \"*/\\.*\", \"-print0\", \"|\", \"LC_ALL=C\", \"sort\", \"-z\", \"|\", \"xargs\", \"-0\", \"sha256sum\", \"|\", \"sha256sum\"]\n",
    "    \n",
    "    process = subprocess.Popen(\" \".join(hash_command), stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "    output, error = process.communicate()\n",
    "    \n",
    "    if error:\n",
    "        print(f\"Error calculating hash: {error.decode()}\")\n",
    "        return None\n",
    "    \n",
    "    return output.decode().strip()\n",
    "\n",
    "def compare_directories(dir1, dir2):\n",
    "    differences = []\n",
    "\n",
    "    def compare_recursive(d1, d2, relpath=\"\"):\n",
    "        dcmp = filecmp.dircmp(d1, d2)\n",
    "        \n",
    "        for name in dcmp.left_only:\n",
    "            differences.append(f\"Only in {d1} (Clone): {os.path.join(relpath, name)}\")\n",
    "        for name in dcmp.right_only:\n",
    "            differences.append(f\"Only in {d2} (Copy): {os.path.join(relpath, name)}\")\n",
    "        \n",
    "        for name in dcmp.diff_files:\n",
    "            path1, path2 = os.path.join(d1, name), os.path.join(d2, name)\n",
    "            if os.path.islink(path1) != os.path.islink(path2):\n",
    "                differences.append(f\"Symbolic link mismatch: {os.path.join(relpath, name)}\")\n",
    "            elif os.path.islink(path1):\n",
    "                if os.readlink(path1) != os.readlink(path2):\n",
    "                    differences.append(f\"Symbolic link target mismatch: {os.path.join(relpath, name)}\")\n",
    "            else:\n",
    "                stat1, stat2 = os.stat(path1), os.stat(path2)\n",
    "                if stat1.st_mode != stat2.st_mode:\n",
    "                    differences.append(f\"File mode mismatch for {os.path.join(relpath, name)}: {stat.filemode(stat1.st_mode)} vs {stat.filemode(stat2.st_mode)}\")\n",
    "                elif stat1.st_size != stat2.st_size:\n",
    "                    differences.append(f\"File size mismatch for {os.path.join(relpath, name)}: {stat1.st_size} vs {stat2.st_size}\")\n",
    "                else:\n",
    "                    differences.append(f\"Content mismatch: {os.path.join(relpath, name)}\")\n",
    "        \n",
    "        for sub_dir in dcmp.common_dirs:\n",
    "            new_relpath = os.path.join(relpath, sub_dir)\n",
    "            compare_recursive(os.path.join(d1, sub_dir), os.path.join(d2, sub_dir), new_relpath)\n",
    "\n",
    "    compare_recursive(dir1, dir2)\n",
    "    return differences\n",
    "\n",
    "def main():\n",
    "    github_slug = \"NemProject/nem\"\n",
    "    commit_hash = \"489aad6060454d0b7b34a144e0b345c5a3a199f5\"\n",
    "    \n",
    "    print(\"Cloning repository...\")\n",
    "    clone_time, clone_path = clone_repo(github_slug, commit_hash)\n",
    "    print(f\"Time taken to clone: {clone_time:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nCopying folder...\")\n",
    "    copy_time, copy_path = copy_folder(clone_path, commit_hash)\n",
    "    print(f\"Time taken to copy: {copy_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\nCloning is {'faster' if clone_time < copy_time else 'slower'} than copying.\")\n",
    "    \n",
    "    print(\"\\nCalculating and comparing directory hashes (excluding hidden files and .git directory)...\")\n",
    "    clone_hash = get_directory_hash(clone_path)\n",
    "    copy_hash = get_directory_hash(copy_path)\n",
    "    \n",
    "    print(f\"Clone hash: {clone_hash}\")\n",
    "    print(f\"Copy hash: {copy_hash}\")\n",
    "    \n",
    "    if clone_hash == copy_hash:\n",
    "        print(\"The contents of the cloned and copied repositories match completely (excluding hidden files and .git directory).\")\n",
    "    else:\n",
    "        print(\"The contents of the cloned and copied repositories do not match.\")\n",
    "        print(\"\\nDetailed comparison:\")\n",
    "        differences = compare_directories(clone_path, copy_path)\n",
    "        for diff in differences:\n",
    "            print(diff)\n",
    "\n",
    "    # Clean up temporary directories\n",
    "    shutil.rmtree(clone_path)\n",
    "    shutil.rmtree(copy_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
